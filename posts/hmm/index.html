<!DOCTYPE html>
<html><head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>「Study Notes」隐马尔可夫模型 - Archie</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="本文主要整理自shuhuai008大佬的白班推导的版书和《统计学习方法》第九章HMM" />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="「Study Notes」隐马尔可夫模型" />
<meta property="og:description" content="本文主要整理自shuhuai008大佬的白班推导的版书和《统计学习方法》第九章HMM" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://athul.github.io/archie/posts/hmm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-27T18:20:50+00:00" />
<meta property="article:modified_time" content="2020-02-27T18:20:50+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="「Study Notes」隐马尔可夫模型"/>
<meta name="twitter:description" content="本文主要整理自shuhuai008大佬的白班推导的版书和《统计学习方法》第九章HMM"/>
<script src="https://athul.github.io/archie/js/feather.min.js"></script>
	
	
        <link href="https://athul.github.io/archie/css/fonts.b685ac6f654695232de7b82a9143a46f9e049c8e3af3a21d9737b01f4be211d1.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://athul.github.io/archie/css/main.40ca3a860425083862b7ebd55447caec5c4384573f0cb098b8d06a91e8dace2e.css" />
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://athul.github.io/archie/">Archie</a>
	</div>
	<nav>
		
		<a href="/archie/">Home</a>
		
		<a href="/archie/about">About</a>
		
		<a href="/archie/posts">All posts</a>
		
		<a href="/archie/tags">Tags</a>
		
		
	</nav>

	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
	
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">「Study Notes」隐马尔可夫模型</h1>
			<div class="meta">Posted on Feb 27, 2020</div>
		</div>
		

		<section class="body">
			<p>本文主要整理自shuhuai008大佬的<a href="https://space.bilibili.com/97068901">白班推导</a>的版书和《统计学习方法》第九章HMM</p>
<!-- raw HTML omitted -->
<h3 id="基本概念">基本概念</h3>
<p>HMM(隐马尔可夫模型)作为一种常见的序列建模的方法，</p>
<p>在隐马尔可夫模型中，我们对存在隐状态与观测值的</p>
<p>HMM中的参数为三元组$\lambda = (\pi,A,B)$ 其中$A$为状态转移矩阵，</p>
<p>$I$为每一时刻的隐状态的集合，$O$为每一时刻观测值的集合，即:</p>
<p>$$I = {i_1,i_2,&hellip;,i_T}, ; O ={o_1,o_2,&hellip;o_T}$$</p>
<p>HMM有两个重要的性质</p>
<p><strong>1.齐次Markov性</strong> 每一时刻的观测值仅仅依赖当前时刻的隐状态
$$
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
$$</p>
<p><strong>2.观测独立性</strong>每一时刻的隐状态仅依赖于其前一时刻
$$
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
$$
HMM的三个基本问题：</p>
<p>1.Evaluation：$p(O|\lambda)$，Forward-Backward 算法</p>
<p>2.Learning：$\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，Baum-Welch 算法</p>
<p>3.Decoding：$I=\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Vierbi 算法</p>
<p>预测问题：$p(i_{t+1}|o_1,o_2,\cdots,o_t)$
​- 2.滤波问题：$p(i_t|o_1,o_2,\cdots,o_t)$</p>
<p>HMM 用概率图表示为：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4</span><span>graph TD;
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5</span><span>t1--&gt;t2;
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6</span><span>subgraph four
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7</span><span>	t4--&gt;x4((x4))
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8</span><span>end
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9</span><span>subgraph three
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10</span><span>	t3--&gt;x3((x3))
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11</span><span>end
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12</span><span>subgraph two
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13</span><span>	t2--&gt;x2((x2))
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14</span><span>end
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15</span><span>subgraph one
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16</span><span>	t1--&gt;x1((x1))
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17</span><span>end
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19</span><span>t2--&gt;t3;
</span></span><span style="display:flex;"><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20</span><span>t3--&gt;t4;
</span></span></code></pre></div><h3 id="evaluation问题">Evaluation问题</h3>
<p>所谓Evaluation问题，就是假定我们已知$\lambda$时，评价任意序列$O$产生的概率，也就是求条件概率$P(O|\lambda)$</p>
<p>直觉上看，既然$\lambda$已知，那么根据观测独立性，我们只要知道隐状态序列$I = {i_1,i_2,&hellip;,i_T}$，就可以进而求出$O ={o_1,o_2,&hellip;o_T}$，所以有:</p>
<p>$$
P(O|\lambda) = P(I|\lambda)P(O|I,\lambda)
$$</p>
<p>我们分开来看
$$
p(I|\lambda)=p(i_1,i_2,\cdots,i_t|\lambda)=p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)
$$
根据Markov假设:
$$
p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)=p(i_t|i_{t-1})=a_{i_{t-1}i_t}
$$
所以有：
$$
p(I|\lambda)=\pi_1\prod\limits_{t=2}^Ta_{i_{t-1},i_t}
$$
再看第二部分
$$
p(O|I,\lambda)=\prod\limits_{t=1}^Tb_{i_t}(o_t)
$$</p>
<p>$$
p(O|\lambda)=\sum\limits_{I}\pi_{i_1}\prod\limits_{t=2}^Ta_{i_{t-1},i_t}\prod\limits_{t=1}^Tb_{i_t}(o_t)
$$</p>
<p>上式中的$\sum\limits_I$  实际上是在对每一步的i求和，包含了$N^T$种轨迹，因此上面这个定义式是一个复杂度为$O(TN^T)$的算式，这种指数复杂度肯定是不能硬解了，这时就该我们的前后向算法登场了。</p>
<h4 id="前向算法">前向算法</h4>
<p>为了方便，我们不妨计$\alpha_t(i)=p(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)$，所以$\alpha_T(i)=p(O,i_T=q_i|\lambda)$，根据贝叶斯公式积分即可得到$p(O|\lambda)$
$$
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
$$
不难看出，前向算法的核心就是要推出$\alpha_T$</p>
<p>我们先考虑$\alpha_{t+1}(j)$
$$
\begin{aligned}
\alpha_{t+1}(j)&amp;=p(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j|\lambda)\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j,i_t=q_i|\lambda)\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|o_1,o_2,\cdots,i_{t+1}=q_j,i_t=q_i|\lambda)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)
\end{aligned}
$$
因为观测独立性假设，所以$o_{t+1}$仅与$i_{t+1}$有关
$$
\begin{aligned}
\alpha_{t+1}(j)&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|o_1,\cdots,o_t,i_t=q_i,\lambda)p(o_1,\cdots,o_t,i_t=q_i|\lambda)\
&amp;=[\sum\limits_{i=1}^Na_{ij}\alpha_t(i)]b_{j}(o_t)
\end{aligned}
$$
到此，我们就得到了$\alpha$的递推公式,而$\alpha_1(i)=\pi_ib_i(o_1)$</p>
<p>在前向算法中，每一次递推的复杂度是N，一共进行T次递推，最后再将$\alpha_T$进行一次积分，复杂度是N，所以前向算法的时间复杂度是$O(TN^2)$</p>
<h4 id="后向算法">后向算法</h4>
<p>与前向算法类似，我们计$\beta_t(i)=p(o_{t+1},o_{t+1},\cdots，o_T|i_t=i,\lambda)$，接下来我们用$\beta$来表示$P(O|\lambda)$
$$
\begin{aligned}p(O|\lambda)&amp;=p(o_1,\cdots,o_T|\lambda)\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T,i_1=q_i|\lambda)\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\
&amp;=\sum\limits_{i=1}^Np(o_1|o_2,\cdots,o_T,i_1=q_i,\lambda)p(o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\
&amp;=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)
\end{aligned}
$$
和前向算法一样，接下来我们推导$\beta$递推式：
$$
\begin{aligned}\beta_t(i)&amp;=p(o_{t+1},\cdots,o_T|i_t=q_i)\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},o_{t+2},\cdots,o_T,i_{t+1}=q_j|i_t=q_i)\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,i_t=q_i)p(i_{t+1}=q_j|i_t=q_i)\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j)a_{ij}\
&amp;=\sum\limits_{j=1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}=q_j)p(o_{t+2},\cdots,o_T|i_{t+1}=q_j)a_{ij}\
&amp;=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)
\end{aligned}
$$
不难看出，后向算法的复杂度同样是$O(N^2T)$</p>
<h3 id="learning问题">Learning问题</h3>
<p>说起learning，本质就是对通过<strong>最大似然法</strong>（MLE）对参数$ \lambda $的学习过程。而这种含有隐状态的模型，自然而然的就想到用EM算法进行优化，下面我们回忆一下EM算法。</p>
<p>EM的最终目的是为了解决含有隐变量的混合模型的参数估计问题即
$$
\theta_{MLE}=\mathop{argmax}\limits_\theta\log p(x|\theta)
$$
它主要由不断迭代的E-step和M-step组成</p>
<p><strong>E-step:</strong> 计算 $\log p(x,z|\theta)$ 在概率分布 $p(z|x,\theta^t)$ 下的期望</p>
<p>**M-step:**计算使这个期望最大化的参数得到$p(z|x,\theta^{（t+1）})$</p>
<p>EM算法可写作：
$$
\theta^{t+1}=\mathop{argmax}<em>{\theta}\int_z\log p(X,Z|\theta)p(Z|X,\theta^t)dz
$$
在HMM中，即为
$$
\begin{aligned}
\lambda^{(t+1)} &amp;= arg \max \limits</em>{\lambda}\sum\limits_{I} \log p(O,I|\lambda)p(I|O,\lambda^{t})
\end{aligned}
$$</p>
<p>其中由于后验项$p(I|O,\lambda^{t})$中不含$\lambda$,因此对上式中$\lambda$的取值是可有可无的，所以可将原式改写为
$$
\begin{aligned}
\lambda^{t+1} &amp;= arg \max \limits_{\lambda}\sum\limits_{I} \log p(O,I|\lambda)p(O,I|\lambda^{t})
\end{aligned}
$$
我们下面先对$\pi^{t+1}$进行参数估计：
$$
\pi^{t+1}=\mathop{argmax}<em>\pi\sum\limits</em>{T}[\log \pi_{i_1}\cdot p(O,i_1&hellip;i_T|\lambda^t)]
$$
首先将联合概率化为边缘概率
$$
\pi^{t+1}=\mathop{argmax}<em>\pi\sum\limits</em>{i}[\log \pi_{i}\cdot p(O,i_1 = q_i|\lambda^t)]
$$
到这一步，就要用上一些技巧了。其实对于$\pi$，是有一个约束条件的，即$\sum\limits_i\pi_i=1$ ，有了约束条件，我们就可以用拉格朗日法进行求解了。</p>
<p>先定义 Lagrange 函数：
$$
L(\pi,\eta)=\sum\limits_{i=1}^N\log \pi_i\cdot p(O,i_1=q_i|\lambda^t)+\eta(\sum\limits_{i=1}^N\pi_i-1)
$$
对$\pi_i$求偏导：
$$
\frac{\partial L}{\partial\pi_i}=\frac{1}{\pi_i}p(O,i_1=q_i|\lambda^t)+\eta=0
$$
自然而然的就可得到：
$$
\pi_i^{t+1}=\frac{p(O,i_1=q_i|\lambda^t)}{p(O|\lambda^t)}
$$
对$A^{t+1}$和$B^{t+1}$也是类似的方法，只是计算会更加繁琐一些。</p>
<p>以上就是HMM中的EM算法，也叫<strong>Baum-Welch</strong>算法。</p>
<h3 id="decoding问题">Decoding问题</h3>
<p>所谓decoding问题，就是在已知观测序列的情况下，预测隐状态序列即$P(I|O)$
$$
I=\mathop{argmax}\limits_{I}P(I|O,\lambda)
$$
实质上就是要求我们找到一个序列,使其概率最大，用老师的话讲，就是在参数空间中找到一条最短路径，这实质上就可以转化成动态规划问题来求解，也就是著名的<strong>维特比算法</strong>（Viterbi algorithm）</p>
<p>首先定义$\delta$，它表征的是，$i_t$已知时，到达$i_t$的最大概率
$$
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)
$$
由于观测独立性和其次马尔可夫性，能得到$\delta$的递推式
$$
\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})
$$
但$\delta$只是一个概率值，我们还需要定义$\psi$来记录节点，$\psi_{t+1}$表示了在前面的路径都已知时，下一时刻最可能到达的隐状态
$$
\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}
$$</p>
<ul>
<li>q</li>
<li>q</li>
</ul>
<h3 id="reference">Reference</h3>
<p><a href="https://anxiang1836.github.io/2019/11/05/NLP_From_HMM_to_CRF/">【NLP】从隐马尔科夫到条件随机场</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6955871.html">隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率</a></p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/archie/tags/nlp">NLP</a></li>
					
					<li><a href="/archie/tags/machine-learning">Machine Learning</a></li>
					
					<li><a href="/archie/tags/study-notes">study notes</a></li>
					
				</ul>
			</nav>
			
			
		</div>
	</article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/athul/archie" title="GitHub"><i data-feather="github"></i></a><a class="soc" href="https://twitter.com/athulcajay/" title="Twitter"><i data-feather="twitter"></i></a><a class="soc" href="https://gitlab.com/athul/" title="GitLab"><i data-feather="gitlab"></i></a></div>
  <div class="footer-info">
    2022  © Sum |  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


<script>
  feather.replace()
</script></div>
    </body>
</html>
