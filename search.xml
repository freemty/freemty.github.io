<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[「Paper-Reading」YOLO Family]]></title>
      <url>/2021/04/30/YOLO/</url>
      <content type="html"><![CDATA[<p>YOLO,You Only Look Once!</p>
<a id="more"></a>
<p>站在2021年这个时间点上,目标检测领域的模型已经百花齐放,其中各个版本的YOLO几乎是最快捷的默认选项.尽管能跑通训练样例,但对我而言,YOLO的内在原理仍然是黑盒,那就从精读YOLOV1开始吧!</p>
<p><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">YOLOV1原文链接</a></p>
<p>当下目标检测比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN）,此类模型一般由两部分构成,其在工作时需要先使用Selective search或RPN找到图中的“可疑区域”(Region Proposal)，然后再在Region Proposal上做分类。而另一类以YOLO为代表的one-stage算法，即仅使用一个CNN网络直接预测不同目标的类别与位置。</p>
<p>不难猜测,R-CNN系列的模型有更高的精度准确,但是速度慢，相应的YOLO家族的特点就是是速度快，但是准确性要更低(《You Only Look Once》这这个名字就很好的体现了YOLO的特点)。</p>
<h4 id="设计思路">设计思路</h4>
<p>具体来说，Yolo的CNN网络将输入的图片分割成 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S" alt="[公式]"> 网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图6所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测 <img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> 个边界框（bounding box）以及边界框的置信度（confidence score）。所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29" alt="[公式]"> ，当该边界框是背景时（即不包含目标），此时 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%3D0" alt="[公式]"> 。而当该边界框包含目标时， <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%3D1" alt="[公式]"> 。边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> 。因此置信度可以定义为 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%2A%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> 。很多人可能将Yolo的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。边界框的大小与位置可以用4个值来表征： <img src="https://www.zhihu.com/equation?tex=%28x%2C+y%2Cw%2Ch%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 是边界框的中心坐标，而 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 是边界框的宽与高。还有一点要注意，中心坐标的预测值 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图6所示。而边界框的 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]"> 范围。这样，每个边界框的预测值实际上包含5个元素： <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%2Cw%2Ch%2Cc%29" alt="[公式]"> ，其中前4个表征边界框的大小与位置，而最后一个值是置信度。</p>
<h4 id="architecture">Architecture</h4>
<p>相比于 YOLO-V1的模型结构不算复杂</p>
<p><img src="/2021/04/30/YOLO/Users/sum_young/private_code/blog/source/_posts/YOLO/architect.png" alt="architect" style="zoom:50%;"></p>
<h4 id="reference">Reference</h4>
<ul>
<li><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32525231" target="_blank" rel="noopener">目标检测|YOLO原理与实现</a></li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> paper reading </tag>
            
            <tag> CV </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Words about ancient history]]></title>
      <url>/2020/06/02/interestingwords/</url>
      <content type="html"><![CDATA[<p>最近痴迷词源，打算开一个说文解字环节</p>
<a id="more"></a>
<h5 id="cicerone博物馆导游"><code>cicerone</code>  (博物馆)导游</h5>
<blockquote>
<p>cicerone A guide, especially one who takes tourists to museums, monuments, or architectural sites and explains what is being seen.</p>
</blockquote>
<p>  西塞罗（Cicero），熟悉罗马历史的人自然不会陌生，著名的雄辩家、政治家。他在行政与辩论时总是风度翩翩且也满腹经纶，因而我们会称那些博物馆的的为<code>cicerone</code>(和西塞罗一样懂的人)。<br></p>
<h5 id="stoic坚韧的苦修的"><code>stoic</code>  坚韧的、苦修的</h5>
<blockquote>
<p>The Stoics were members of a philosophical movement that first appeared in ancient Greece and lasted well into the Roman era. Stoicism taught that humans should seek to free themselves from joy, grief, and passions of all kinds in order to attain wisdom; its teachings thus have much in common with Buddhism.</p>
</blockquote>
<p>  <code>stoic</code>源于斯多葛主义（Stoicism），斯多葛学派认为我们要持戒，远离享乐以追求有德行的生活。最著名的斯多葛主义者有西塞罗（Cirero)、罗马皇帝马可·奥勒留（Marcus Aurelius）</p>
<blockquote>
<p>e.g. She bore the pain of her broken leg with such stoic patience that most of us had no idea she was suffering.</p>
</blockquote>
<h5 id="hedonism-享乐主义"><code>hedonism</code> 享乐主义</h5>
<blockquote>
<p>An attitude or way of life based on the idea that pleasure or happiness should be the chief goal.</p>
</blockquote>
<p>  <code>hedonism</code>的词源是希腊语的&quot;ἡδονή（hēdonē）&quot;（享乐）加上后缀&quot;ισμός（ismos）&quot;（主义）<br> 说到<code>hedonism</code> 就不得不提伊壁鸠鲁主义（Epicureanism）。希腊哲学家伊比鸠鲁认为，人生追根究底不过是为了享受快乐。不过，这里的快乐并不是指性爱跟毒品，而是没有痛苦的。他认为快乐有两种，一种是短暂的，另一种是持久的。短暂的快乐是从满足需求而来，口渴的人喝到水会感觉到快乐，疲倦的人躺下休息也会感到快乐。除了这种短暂的快乐之外，还有一种持久的快乐，而这才会带来真正幸福的生活。这种持久的快乐，指的是没有痛苦，也没有强烈匮乏的状态。<br></p>
<blockquote>
<p>e.g. In her new spirit of hedonism, she went out for a massage, picked up champagne and chocolate truffles, and made a date that evening with an old boyfriend.</p>
</blockquote>
<h5 id="delphic模糊的暧昧的"><code>delphic</code>  模糊的、暧昧的</h5>
<blockquote>
<p>Unclear, ambiguous, or confusing.</p>
</blockquote>
<p>  德尔菲（Delphi）是希腊阿波罗神庙的所在地，祭司会在这里接受神喻以占卜吉凶。以其神喻的晦涩暧昧而有的这个词。<br>  神喻这和算命一个道理，总是给出的都是看似富有哲理，实则模棱两可的答案（这也是”神“维持一种卡里斯玛的必要手段）。吕底亚（Lydia）国王，曾来问询波斯是否会攻打自己的国家，神喻说“会有一个庞大的帝国覆灭”，国王自然以为所指的是波斯，谁知最终覆灭的却是他自己的帝国。<br></p>
<blockquote>
<p>​ e.g. All she could get from the strange old woman were a few delphic comments that left her more confused than ever about the missing documents.</p>
</blockquote>
<h5 id="thespian演员"><code>thespian</code>  演员</h5>
<blockquote>
<p>actor</p>
</blockquote>
<p>  公元前六世纪的泰斯庇斯（Thespis）是古希腊“悲剧”（Tragedy）的开创者。最初，古希腊的戏剧都是歌队的合唱而他第一次将酒神祭典上的合唱改写成了对话剧本，将表演引入了戏剧，并且也是由他本人初次表演自己创作的悲剧剧本。<br>  自他以后希腊才有了真正意义上的戏剧，而他也第一位是真正意义上actor，也难怪人们会用<code>thespian</code>来指代演员。<br></p>
<blockquote>
<p>​ e.g. In summer the towns of New England welcome troupes of thespians dedicated to presenting plays of all kinds.</p>
</blockquote>
<h4 id="words-from-homeric">Words from Homeric</h4>
<h5 id="hector威吓的"><code>hector</code>  威吓的</h5>
<blockquote>
<p>To talk and behave towards someone in a loud and unpleasantly forceful way, especially in order to get them to act or think as you want</p>
</blockquote>
<p>  在历史上，赫克托耳（Hector）是特洛伊王子，帕里斯的哥哥。特洛伊第一勇士，被称为“特洛伊的城墙”，勇冠三军，为人正直，是古希腊传说中的的英雄形象，最终被阿克琉斯（Achilles）所杀。<br>  但在《伊利亚特》中，似乎荷马并不是很喜欢赫克托耳，赫克托尔被描写为嗜杀。英语中似乎继承了这种观点。<code>hector</code>有威吓，恃强凌弱，虚张声势的贬义.</p>
<blockquote>
<p>e.g. He would swagger around the apartment entrance with his friends and hector the terrified inhabitants going in and out.</p>
</blockquote>
<h5 id="stentorian大声的洪亮的"><code>stentorian</code>  大声的、洪亮的</h5>
<blockquote>
<p>Stentor, like Hector, was a warrior in the Iliad, but on the Greek side. His unusually powerful voice (Homer calls him “brazen-voiced”—that is, with a voice like a brass instrument) made him the natural choice for delivering announcements and proclamations to the assembled Greek army, in an era when there was no way of artificially increasing the volume of a voice.</p>
</blockquote>
<p>  斯藤托耳（Stentor）是特洛伊战争中联军的传令官，以其大嗓门而闻名<br></p>
<blockquote>
<p>e.g. Even without a microphone, his stentorian voice was clearly audible in the last rows of the auditorium.</p>
</blockquote>
<h5 id="nestor-睿智的长者"><code>nestor</code>   睿智的长者</h5>
<blockquote>
<p>Nestor was another character from the Iliad, the eldest of the Greek leaders in the Trojan War. A great warrior as a young man, he was now noted for his wisdom and his talkativeness, both of which increased as he aged.</p>
</blockquote>
<p>  涅斯托尔（Nestor）是皮洛斯国王涅琉斯的儿子，也是涅琉斯的12个儿子中唯一未被赫拉克勒斯杀死的幸存者。而在《伊利亚特》中，涅斯托尔是联军中一位睿智、长寿的领袖，常用自己的智慧与阅历来激励年轻的战士。<br>  在英语语境中，<code>nestor</code>指的是那种阅历丰富，能给出建议且不说教（爹味儿不重）的长者。</p>
<blockquote>
<p>e.g. He would swagger around the apartment entrance with his friends and hector the terrified inhabitants going in and out.</p>
</blockquote>
<h4 id="words-from-the-gods-of-athens">Words from the gods of Athens</h4>
<h5 id="dionysian-冲动的狂欢的"><code>Dionysian</code> 冲动的、狂欢的</h5>
<blockquote>
<p>Frenzied, delirious..</p>
</blockquote>
<p>  狄俄尼索斯（Dionysus），是希腊神话中的酒神与剧作之神。熟悉尼采对酒神精神肯定不会陌生。酒神有着极其丰富的精神意涵。在许多哲学与文学（尤其是尼采）表述中，酒神与日神被视作两种对立的精神。最浅显的讲，日神精神的核心是美的外观，是节制有序和理性，它指涉的是形式主义和古典主义、视觉艺术。而酒神精神更多是一种痛苦与狂喜快乐交织的状态，大都是非理性或需要借助极端体验来达成的，它指涉的是浪漫主义、音乐和表演艺术。<br>  酒神精神的来源是希腊的酒神祭，在酒神祭中，人们打破禁忌、放纵欲望，解除一切束缚，复归自然。酒神状态的迷狂，它对人生日常界线和规则的破坏，期间，包含着一种恍惚的成分，个人过去所经历的一切都淹没在其中了。是一种狂热、疯狂的快感、是人与人之间的界限消弥。<br></p>
<h5 id="apollonian-理性的和谐的"><code>Apollonian</code> 理性的、和谐的</h5>
<blockquote>
<p>Harmonious, ordered, rational, calm.</p>
</blockquote>
<p>  阿波罗（Apollo）作为日神。由于尼采对”日神精神“的强调，阿波罗作为酒神的反面，成为了理性、冷静、秩序的象征。还有一个同义词<code>bacchanalian</code>，同样是指酒神式的，不过词源是罗马的酒神 <br></p>
<blockquote>
<p>e.g. After a century of Romantic emotion, some composers adopted a more Apollonian style, producing clearly patterned pieces that avoided extremes of all kinds.</p>
</blockquote>
<h5 id="venereal-性病性交的"><code>venereal</code> 性病、性交的</h5>
<blockquote>
<p>Having to do with sexual intercourse or diseases transmitted by it.</p>
</blockquote>
<p>  维纳斯（Venus）是古罗马爱神的名字，在希腊神话中牠的名字是阿佛洛狄忒（Aphrodite）。有趣的是，爱神的希腊名字和罗马名字分别是金星（Venus）和星期五（Firday）的词源。<br></p>
<blockquote>
<p>e.g. In the 19th century syphilis especially was often fatal, and venereal diseases killed some of the greatest figures of the time.</p>
</blockquote>
<h5 id="mercurial-神速的不稳定的"><code>mercurial</code> 神速的、不稳定的</h5>
<blockquote>
<p>Having rapid and unpredictable changes of mood</p>
</blockquote>
<p>  罗马神墨丘利（Mercury）也就是我们熟知的赫尔墨斯（Hermes）是掌管旅行之神，他行走如飞，因而成为神速的代名词。而水星被罗马人命名为Mercury，也是因为它的移动快<br></p>
<blockquote>
<p>​ e.g. His mother's always mercurial temper became even more unpredictable, to the point where the slightest thing would trigger a violent fit.</p>
</blockquote>
<h5 id="jovial-愉悦"><code>jovial</code> 愉悦</h5>
<blockquote>
<p>Jolly, good-natured.</p>
</blockquote>
<p>  罗马神朱庇特（ Jupiter or Jove），就是众神之王宙斯（Zeus）。因为其地位高，罗马人也将他们已知的最大的行星木星命名为<code>Jupiter</code>。占星术从东方传到罗马帝国时，占星家宣称那些“生于木星下”的人注定要快乐大方，因而这也成了“愉悦”的代名词，这也也是<code>-joy</code>这个词根的来源。<br> 或许是英语受拉丁语影响更大的缘故，这些词源都是罗马神的名字而非希腊神话中的本名</p>
<blockquote>
<p>e.g. Their grandfather was as jovial and sociable as their grandmother was quiet and withdrawn</p>
</blockquote>
<h5 id="muse-沉思冥想"><code>muse</code> 沉思、冥想</h5>
<blockquote>
<p>A source of inspiration; a guiding spirit.</p>
</blockquote>
<p>  缪斯（The Muse），是希腊九位女神的总称，他们是音乐与艺术之神，她们喜欢歌手，所以会赐予她们灵感</p>
<blockquote>
<p>At 8:00 each morning he sat down at his desk and summoned his muse, and she almost always responded.</p>
</blockquote>
]]></content>
      
        
        <tags>
            
            <tag> English </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[隐马尔可夫模型]]></title>
      <url>/2020/02/27/HMM/</url>
      <content type="html"><![CDATA[<p>本文主要整理自shuhuai008大佬的<a href="https://space.bilibili.com/97068901" target="_blank" rel="noopener">白班推导</a>的版书和《统计学习方法》第九章HMM</p>
<a id="more"></a>
<h3 id="基本概念">基本概念</h3>
<p>HMM(隐马尔可夫模型)作为一种常见的序列建模的方法，</p>
<p>在隐马尔可夫模型中，我们对存在隐状态与观测值的</p>
<p>HMM中的参数为三元组<span class="math inline">\(\lambda = (\pi,A,B)\)</span> 其中<span class="math inline">\(A\)</span>为状态转移矩阵，</p>
<p><span class="math inline">\(I\)</span>为每一时刻的隐状态的集合，<span class="math inline">\(O\)</span>为每一时刻观测值的集合，即:</p>
<p><span class="math display">\[I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}\]</span></p>
<p>HMM有两个重要的性质</p>
<p><strong>1.齐次Markov性</strong> 每一时刻的观测值仅仅依赖当前时刻的隐状态 <span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p>
<p><strong>2.观测独立性</strong>每一时刻的隐状态仅依赖于其前一时刻 <span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span> HMM的三个基本问题：</p>
<p>1.Evaluation：<span class="math inline">\(p(O|\lambda)\)</span>，Forward-Backward 算法</p>
<p>2.Learning：<span class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)\)</span>，Baum-Welch 算法</p>
<p>3.Decoding：<span class="math inline">\(I=\mathop{argmax}\limits_{I}p(I|O,\lambda)\)</span>，Vierbi 算法</p>
<p>预测问题：<span class="math inline">\(p(i_{t+1}|o_1,o_2,\cdots,o_t)\)</span> ​- 2.滤波问题：<span class="math inline">\(p(i_t|o_1,o_2,\cdots,o_t)\)</span></p>
<h3 id="evaluation问题">Evaluation问题</h3>
<p>所谓Evaluation问题，就是假定我们已知<span class="math inline">\(\lambda\)</span>时，评价任意序列<span class="math inline">\(O\)</span>产生的概率，也就是求条件概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>直觉上看，既然<span class="math inline">\(\lambda\)</span>已知，那么根据观测独立性，我们只要知道隐状态序列<span class="math inline">\(I = \{i_1,i_2,...,i_T\}\)</span>，就可以进而求出<span class="math inline">\(O =\{o_1,o_2,...o_T\}\)</span>，所以有:</p>
<p><span class="math display">\[
P(O|\lambda) = P(I|\lambda)P(O|I,\lambda)
\]</span></p>
<p>我们分开来看 <span class="math display">\[
p(I|\lambda)=p(i_1,i_2,\cdots,i_t|\lambda)=p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)
\]</span> 根据Markov假设: <span class="math display">\[
p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)=p(i_t|i_{t-1})=a_{i_{t-1}i_t}
\]</span> 所以有： <span class="math display">\[
p(I|\lambda)=\pi_1\prod\limits_{t=2}^Ta_{i_{t-1},i_t}
\]</span> 再看第二部分 <span class="math display">\[
p(O|I,\lambda)=\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span></p>
<p><span class="math display">\[
p(O|\lambda)=\sum\limits_{I}\pi_{i_1}\prod\limits_{t=2}^Ta_{i_{t-1},i_t}\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span></p>
<p>上式中的<span class="math inline">\(\sum\limits_I\)</span> 实际上是在对每一步的i求和，包含了<span class="math inline">\(N^T\)</span>种轨迹，因此上面这个定义式是一个复杂度为<span class="math inline">\(O(TN^T)\)</span>的算式，这种指数复杂度肯定是不能硬解了，这时就该我们的前后向算法登场了。</p>
<h4 id="前向算法">前向算法</h4>
<p>为了方便，我们不妨计<span class="math inline">\(\alpha_t(i)=p(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\)</span>，所以<span class="math inline">\(\alpha_T(i)=p(O,i_T=q_i|\lambda)\)</span>，根据贝叶斯公式积分即可得到<span class="math inline">\(p(O|\lambda)\)</span> <span class="math display">\[
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
\]</span> 不难看出，前向算法的核心就是要推出<span class="math inline">\(\alpha_T\)</span></p>
<p>我们先考虑<span class="math inline">\(\alpha_{t+1}(j)\)</span> <span class="math display">\[
\begin{aligned}
\alpha_{t+1}(j)&amp;=p(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j|\lambda)\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j,i_t=q_i|\lambda)\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|o_1,o_2,\cdots,i_{t+1}=q_j,i_t=q_i|\lambda)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)
\end{aligned}
\]</span> 因为观测独立性假设，所以<span class="math inline">\(o_{t+1}\)</span>仅与<span class="math inline">\(i_{t+1}\)</span>有关 <span class="math display">\[
\begin{aligned}
\alpha_{t+1}(j)&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|o_1,\cdots,o_t,i_t=q_i,\lambda)p(o_1,\cdots,o_t,i_t=q_i|\lambda)\\
&amp;=[\sum\limits_{i=1}^Na_{ij}\alpha_t(i)]b_{j}(o_t)
\end{aligned}
\]</span> 到此，我们就得到了<span class="math inline">\(\alpha\)</span>的递推公式,而<span class="math inline">\(\alpha_1(i)=\pi_ib_i(o_1)\)</span></p>
<p>在前向算法中，每一次递推的复杂度是N，一共进行T次递推，最后再将<span class="math inline">\(\alpha_T\)</span>进行一次积分，复杂度是N，所以前向算法的时间复杂度是<span class="math inline">\(O(TN^2)\)</span></p>
<h4 id="后向算法">后向算法</h4>
<p>与前向算法类似，我们计<span class="math inline">\(\beta_t(i)=p(o_{t+1},o_{t+1},\cdots，o_T|i_t=i,\lambda)\)</span>，接下来我们用<span class="math inline">\(\beta\)</span>来表示<span class="math inline">\(P(O|\lambda)\)</span> <span class="math display">\[
\begin{aligned}p(O|\lambda)&amp;=p(o_1,\cdots,o_T|\lambda)\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T,i_1=q_i|\lambda)\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\\
&amp;=\sum\limits_{i=1}^Np(o_1|o_2,\cdots,o_T,i_1=q_i,\lambda)p(o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\\
&amp;=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)
\end{aligned}
\]</span> 和前向算法一样，接下来我们推导<span class="math inline">\(\beta\)</span>递推式： <span class="math display">\[
\begin{aligned}\beta_t(i)&amp;=p(o_{t+1},\cdots,o_T|i_t=q_i)\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},o_{t+2},\cdots,o_T,i_{t+1}=q_j|i_t=q_i)\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,i_t=q_i)p(i_{t+1}=q_j|i_t=q_i)\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j)a_{ij}\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}=q_j)p(o_{t+2},\cdots,o_T|i_{t+1}=q_j)a_{ij}\\
&amp;=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)
\end{aligned}
\]</span> 不难看出，后向算法的复杂度同样是<span class="math inline">\(O(N^2T)\)</span></p>
<h3 id="learning问题">Learning问题</h3>
<p>说起learning，本质就是对通过<strong>最大似然法</strong>（MLE）对参数$ $的学习过程。而这种含有隐状态的模型，自然而然的就想到用EM算法进行优化，下面我们回忆一下EM算法。</p>
<p>EM的最终目的是为了解决含有隐变量的混合模型的参数估计问题即 <span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits_\theta\log p(x|\theta)
\]</span> 它主要由不断迭代的E-step和M-step组成</p>
<p><strong>E-step:</strong> 计算 <span class="math inline">\(\log p(x,z|\theta)\)</span> 在概率分布 <span class="math inline">\(p(z|x,\theta^t)\)</span> 下的期望</p>
<p><strong>M-step:</strong>计算使这个期望最大化的参数得到<span class="math inline">\(p(z|x,\theta^{（t+1）})\)</span></p>
<p>EM算法可写作： <span class="math display">\[
\theta^{t+1}=\mathop{argmax}_{\theta}\int_z\log p(X,Z|\theta)p(Z|X,\theta^t)dz
\]</span> 在HMM中，即为 <span class="math display">\[
\begin{aligned}
\lambda^{(t+1)} &amp;= arg \max \limits_{\lambda}\sum\limits_{I} \log p(O,I|\lambda)p(I|O,\lambda^{t})
\end{aligned}
\]</span></p>
<p>其中由于后验项<span class="math inline">\(p(I|O,\lambda^{t})\)</span>中不含<span class="math inline">\(\lambda\)</span>,因此对上式中<span class="math inline">\(\lambda\)</span>的取值是可有可无的，所以可将原式改写为 <span class="math display">\[
\begin{aligned}
\lambda^{t+1} &amp;= arg \max \limits_{\lambda}\sum\limits_{I} \log p(O,I|\lambda)p(O,I|\lambda^{t})
\end{aligned}
\]</span> 我们下面先对<span class="math inline">\(\pi^{t+1}\)</span>进行参数估计： <span class="math display">\[
\pi^{t+1}=\mathop{argmax}_\pi\sum\limits_{T}[\log \pi_{i_1}\cdot p(O,i_1...i_T|\lambda^t)]
\]</span> 首先将联合概率化为边缘概率 <span class="math display">\[
\pi^{t+1}=\mathop{argmax}_\pi\sum\limits_{i}[\log \pi_{i}\cdot p(O,i_1 = q_i|\lambda^t)]
\]</span> 到这一步，就要用上一些技巧了。其实对于<span class="math inline">\(\pi\)</span>，是有一个约束条件的，即<span class="math inline">\(\sum\limits_i\pi_i=1\)</span> ，有了约束条件，我们就可以用拉格朗日法进行求解了。</p>
<p>先定义 Lagrange 函数： <span class="math display">\[
 L(\pi,\eta)=\sum\limits_{i=1}^N\log \pi_i\cdot p(O,i_1=q_i|\lambda^t)+\eta(\sum\limits_{i=1}^N\pi_i-1)
\]</span> 对<span class="math inline">\(\pi_i\)</span>求偏导： <span class="math display">\[
\frac{\partial L}{\partial\pi_i}=\frac{1}{\pi_i}p(O,i_1=q_i|\lambda^t)+\eta=0
\]</span> 自然而然的就可得到： <span class="math display">\[
\pi_i^{t+1}=\frac{p(O,i_1=q_i|\lambda^t)}{p(O|\lambda^t)}
\]</span> 对<span class="math inline">\(A^{t+1}\)</span>和<span class="math inline">\(B^{t+1}\)</span>也是类似的方法，只是计算会更加繁琐一些。</p>
<p>以上就是HMM中的EM算法，也叫<strong>Baum-Welch</strong>算法。</p>
<h3 id="decoding问题">Decoding问题</h3>
<p>所谓decoding问题，就是在已知观测序列的情况下，预测隐状态序列即<span class="math inline">\(P(I|O)\)</span> <span class="math display">\[
I=\mathop{argmax}\limits_{I}P(I|O,\lambda)
\]</span> 实质上就是要求我们找到一个序列,使其概率最大，用老师的话讲，就是在参数空间中找到一条最短路径，这实质上就可以转化成动态规划问题来求解，也就是著名的<strong>维特比算法</strong>（Viterbi algorithm）</p>
<p>首先定义<span class="math inline">\(\delta\)</span>，它表征的是，<span class="math inline">\(i_t\)</span>已知时，到达<span class="math inline">\(i_t\)</span>的最大概率 <span class="math display">\[
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)
\]</span> 由于观测独立性和其次马尔可夫性，能得到<span class="math inline">\(\delta\)</span>的递推式 <span class="math display">\[
\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})
\]</span> 但<span class="math inline">\(\delta\)</span>只是一个概率值，我们还需要定义<span class="math inline">\(\psi\)</span>来记录节点，<span class="math inline">\(\psi_{t+1}\)</span>表示了在前面的路径都已知时，下一时刻最可能到达的隐状态 <span class="math display">\[
\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}
\]</span></p>
<h3 id="reference">Reference</h3>
<p><a href="https://anxiang1836.github.io/2019/11/05/NLP_From_HMM_to_CRF/" target="_blank" rel="noopener">【NLP】从隐马尔科夫到条件随机场</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6955871.html" target="_blank" rel="noopener">隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率</a></p>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」Transformer——NLP界大魔王的诞生]]></title>
      <url>/2020/02/27/attention/</url>
      <content type="html"><![CDATA[<p>Attention is all you need !!!</p>
<a id="more"></a>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> paper reading </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Personal Project」用StyleGAN来生成浮世绘]]></title>
      <url>/2020/02/27/gan-ukiyo-generator/</url>
      <content type="html"><![CDATA[<p>来玩玩图像生成吧!</p>
<a id="more"></a>
<h4 id="前言">前言</h4>
<p>但在具体了解了之后发现,现在基于GAN的图像生成模型已经相当成熟了,例如Google在2018年已经用BigGAN刷爆了ImageNet上的所有种类图像的生成(如图中BigGAN生成的汉堡)且都已经达到了可以“以假乱真”的水平了。轮子已经造好了,从零开始进行完成一个图像生成方面的模型是几乎不可能也是不必要的,很多原本一厢情愿的“创意”都变得相当的鸡肋。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/hambuger.jpeg" alt="hambuger" style="zoom:25%;"></p>
<p>​ 不过另一方面,这些现成的模型也给了在更精细领域上进行产出成为可能。就例如在Github上的开发者roadrunner01就用动漫头像数据集来微调英伟达训练好的styleGAN人脸生成模型,得到了鼎鼎大名的“二次元老婆生成器”,生成样例如图所示。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/sister.png" style="zoom: 25%;"></p>
<p>​ 受到“小姐姐生成器”的启发,我也开始尝试于通过小数据集来fine-tune已有的生成模型,训练出一个性化领域的图像生成器。最终很幸运从大神Justin Pinkney的博客找到了一个规模在5000张左右的日本浮世绘的人脸数据集,并以此为基础去fine-tune了已有的styleGAN2的人脸生成模型。最终训练得到了“浮世绘”生成器,取得了还算不错的生成结果,部分生成样例如图所示。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/baby_result.jpeg" alt="baby_result" style="zoom:50%;"></p>
<p>​ 当然,这只是第一步,在阅读了styleGAN的论文和相关解读的博客之后,又进一步得到了更有趣的玩法,就是可以通过将两个styleGAN模型中生成器不同风格生成层（style-layer)的参数进行融合中融合,得到混合风格的生成结果。最终通过融合已有的人脸生成模型和浮世绘生成模型,分别得到了浮世绘风格的人脸照片与人脸到对应浮世绘的渐变图,其中部分融合模型生成结果如图。</p>
<p>​ <img src="/2020/02/27/gan-ukiyo-generator/figure/baby_transfer.jpeg" alt="baby_transfer" style="zoom:30%;"></p>
<p>以上的所有的训练与生成结果都部署在了谷歌Colab的在线notebook当中,可以通过浏览器直接运行并复现生成结果,notebook链接在 <a href="https://colab.research.google.com/drive/1fQ1VrYlnKjVahuc1G8slotKXD1O6RnWV" target="_blank" rel="noopener">这里</a></p>
<h4 id="gan的原理补充">GAN的原理补充</h4>
<p>​ 就我的自身理解,图像生成任务（或更广的讲,深度学习的几乎所有任务)是基于一个基本假设展开的,即一切事物的属性都有着其对应着某种分布特征,就好比与其他的图片相比,所有“猫”的图片在灰度值范围或者分布上是有其独特之处的,虽然那必然是一个极其“晦涩”的且难以从直觉上理解的高纬度特征,但深度学习飞速发展的事实证明,只要数据集样本够多且label正确,这样分布特征总是可以被够深的模型所找到。</p>
<p>​ 在初次了解GAN的工作机制时,真的可以说是惊为天人,因为这种运作模式是在是太符合人类的直觉了,更难得的是一个如此符合人类直觉的模型竟然可以真的被实现,还取得如此好的效果。</p>
<p>​ GAN的大致结构如图所示,其主要是由两个神经网络组合而成,分别是生成器Generator(下计为G)和判别器Discriminator（以下计为D)。生成器负责通过根据输入（在图像生成任务中通常是和输出图片等尺寸随机噪音)生成人造样本。之后将生成样本与数据集中的真实样本同时输入到判别器当中。而判别器的本质也就是一个分类器,输出其对于输入样本的判别结果。套用一位博主的比喻,生成器就像是一个假钞的仿造者,而判别器就是一台验钞机,当的生成器仿造的假币可以顺利通过一台足够强大验钞机的时候,我们的生成任务也就达成了。唉,不得不再次感叹GAN这个创意的简洁与强大！</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/GAN.jpg" alt="GAN" style="zoom:50%;"></p>
<p>​ 基于以上的工作原理即可以得到GAN的目标函数。</p>
<p><span class="math display">\[
V(D, G)=
        E_{x\sim p_{data}(x)}[\log D(x)]
        + E_{z\sim p_z(z)}[\log(1 - D(G(z)))]
\]</span> 从公式不难可以看出中当判别器将所有样本的为真概率都输出为0.5时,目标函数可以达到最小值,也就可以理解为判别器已经完全无法分辨真实样本和生成样本,也就可以理解为生成器样本可以达到“以假乱真”的水平(当让这是在判别器也足够强大的前提下)</p>
<h4 id="stylegan的特点">StyleGAN的特点</h4>
<p>StyleGAN是本次实验使用的图像生成模型。它是Nvidia在2018年提出的。在阅读了网上其他博主对论文的解读之后,了解到StyleGAN的整体生成思路实际上是继承于ProGAN(渐进式生成对抗网络)。</p>
<p>​ 而ProGAN的核心在于是其中的渐进式的生成模型。一张高质量的图像（比如<span class="math inline">\(1024\times1024\)</span>的RGB图像)其中含有个过多的特征,直接生成是相当困难的。因此就可以由小至大,逐级生成。也就是,先使用非常低分辨率的图像（如：4×4)开始训练生成器和判别器,并且逐次开始训练生成器和判别器,增加更高分辨率（8×8、16×16...)的网络层,直到最后能产生一个具有丰富细节的高清图片。而这也正是StyleGAN的生成器<span class="math inline">\(G_s\)</span>所采用的基本结构。但既然名字叫做StyleGAN,它的风格控制也就主要体现在每个分辨率的网络层都有一个独立的噪声输入<span class="math inline">\(B\)</span>,可以通过改变不同层的噪声输入来调整各个层级的风格特征,大如性别,年龄,小到眉毛的粗细。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/styleGAN.png" alt="styleGAN" style="zoom:20%;"></p>
<p>​ 除此之外就是StyleGAN中新增的一个由八个线性层构成的输入映射网络<span class="math inline">\(G_m\)</span>,作者说这里是通过映射使得网络生成的图像风格更为统一。模型结构大概就如图左所示,而右图则给出了在训练速度于生成质量上StyleGAN的优胜之处。</p>
<h4 id="浮世绘图像生成">浮世绘图像生成</h4>
<h5 id="ukiyo-e-faces-数据集">Ukiyo-E Faces 数据集</h5>
<p>​ 这次实验用到的是“Ukiyo-e faces dataset”。也是恰好找到了一个可以用且足够有趣的数据集。这是一个浮世绘数据集,是作者Justin Pinkney是从各大博物馆以及相关网站中爬下来的几千张浮世绘图片。但由于数据来源十分广泛,图片的质量和尺寸是千奇百怪,这里为了方便训练,作者专门使用了基于ESRGAN的分辨率增强模型,将所有图片统一到了1024X1024的分辨率,最终得到了一个5024张的浮世绘数据集,数据集的预览如图</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/overview.jpeg" alt="数据集预览图" style="zoom: 25%;"></p>
<p>​ 就直观来看,利用这个浮世绘数据集来微调StyleGAN的可行性还是蛮高的,一来作为艺术作品,它的颜色风格相比于真实世界还是要更为固定一些,数据集中的人像质量都很高,风格也都较为统一,例如男性的发型和面部的朝向大致可以划分为几种常见的风格,面部的颜色也基本相同,此外作为一个微调任务,5000张高质量人像已经算很充足了。</p>
<h5 id="模型结构">模型结构</h5>
<p>​ 由于模型比较大,很难直观的输出参数,不过好在使用的这个英伟达在github上的放出的模型是用Tensorflow写成的,因此可以很直观的使用TF中的Tensofboard可视化的输出整个模型结构。使用Tensorboard输出整个模型如图,可以看到模型主题是由输入映射网络<span class="math inline">\(G_m\)</span>,渐进式生成网络<span class="math inline">\(G_s\)</span>和判别器<span class="math inline">\(D\)</span>所构成的。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/model.png" style="zoom:25%;"></p>
<p>​ 生成器<span class="math inline">\(G_s\)</span>的内部结构如图所示,可以看到生成器中所串联的由<span class="math inline">\(4\times4\)</span>一直到<span class="math inline">\(256 \times 256\)</span>的分辨率不断提高的生成层,此外,每个生成层左侧所额外连接的输入即为StyleGAN中控制图像风格的噪音输入。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/gs.png" style="zoom: 50%;"></p>
<p>​ 这里再输出一下StyleGAN中最重要的部分,就是其用来控制生成图像具体生成细节的风格层,这里以<span class="math inline">\(G_s\)</span>中的128*128层为例,其结构如下图,与论文中的结构图类似（上右图为我根据slim模型结构重新绘制的生成层结构图),每个生成层都是由一个两个卷积网络,一个以及噪声输入的样式模块AdaIN和输出前的归一化层构成。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/layer.png" alt="layer" style="zoom: 70%;"></p>
<h5 id="训练过程">训练过程</h5>
<p>​ 这里参考了其他博主的微调经验之后。我选取了英伟达已经在FFHQ上训练完成的人像生成模型作为基准。在此基础上使用浮世绘数据集进行微调。FFHQ数据如图所示,其中包含了7万张1024分辨率的高清晰度人像。</p>
<figure>
<img src="/2020/02/27/gan-ukiyo-generator/figure/ffhq.jpeg" alt="ffhq"><figcaption>ffhq</figcaption>
</figure>
<p>​ 这里的模型训练为了方便,就直接在英伟达官方放出的基于FFHQ数据集的训练代码上改了一下模型尺寸,然后就用google的COLAB跑起来了。使用的GPU就是一块google的免费提供的Tesla T4,性能基本等于一张RTX2070,算是很良心了。然后预训练模型的参数就选用的是英伟达在FFHQ数据集上训练好的256分辨率的slim结构的模型。</p>
<figure>
<img src="/2020/02/27/gan-ukiyo-generator/figure/process.png" alt="process"><figcaption>process</figcaption>
</figure>
<p>不得不说,GAN的训练确实比较耗算力,即使只是低分辩输入,slim结构的styleGAN模型也训练大概断断续了花了九个小时。</p>
<h5 id="浮世绘风格的现代人像生成">浮世绘风格的现代人像生成</h5>
<p>​ 使用fine-tune好模型的先随机跑了五十张图出来,效果如图[]所示,可以看到生成结果还是相当可观的,至少大多数图片不论是从画风还是人物神态都是“有那味”的（当然也有一些“不成人形”的存在)。不过还是存在很明显的共性缺陷的,例如相当一部分生成人物都是“斗鸡眼”或者翻白眼。这里我猜测一来是算力有限的缘故,选取的参数比较少的slim的结构的StyleGAN,本身的特征捕捉能力就较弱；此外本身浮世绘中人像的眼睛都较小,特征较为难以捕捉,再加上为了适应slim的模型结构,输入图片都被从1024$<span class="math inline">\(1024压缩到了256\)</span>$256,导致模型可以捕捉到的眼睛部分的特征更加稀缺,所以导致了生成结果的眼神都画风清奇。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/results.jpeg" alt="results" style="zoom:50%;"></p>
<h4 id="modelblending-生成浮世绘风格的现代人像">ModelBlending 生成浮世绘风格的现代人像</h4>
<h5 id="模型融合的玩法">模型融合的玩法</h5>
<p>​ 训练完生成模型之后,按理说该做的也就差不多了,可感觉到现在为止自己做的就是找到已有的代码和合适的数据集,然后稍微修改数据集的格式和运行代码,然后就结束了,完全就像是在玩一个黑箱。另一方面费了这么久训练出来的模型,就跑出几个看着还行的生成样例就要吃灰了,也太浪费了！StyleGAN一定还有更有趣的玩法。</p>
<p>​ 结果确实如此,就在提供数据集的博主那里,就找到了一篇关于生成模型model Bledding的方法。大致就是通过融合StyleGAN不同模型中<span class="math inline">\(G_s\)</span>的stylelaye,得到新的模型,从而生成具有混合风格的图像,博客中就给出了网上有人做的二次元小姐姐和小马宝莉的StyleGAN融合模型如图,别说效果还真不错。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/arfa-frames.jpg"></p>
<p>​ 作为一个非常流行的生成模型,StyleGAN的应用起来的一大优势就是有各种数据集上预训练模型可以直接拿过来用。另一方面本身这个浮世绘生成模型就是在FFHQ模型上上微调得到的,因此就可以直接把原本的FFHQ模型参数作为融合对象,这样就有可以得到浮世绘风格的现代人相。（不知道为什么一看到个玩法在我心里立马就蹦出了下图的画面)</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/funny.jpeg" alt="funny" style="zoom:30%;"></p>
<h5 id="model-belnding-的原理">Model Belnding 的原理</h5>
<p>​ 前面已经提到了StyleGAN的核心就是从ProGAN那里继承的渐进式生成网络,其生成的高清图像是从极低分辨率（<span class="math inline">\(4\times4\)</span>)一层一层提升得到的,而相应的,每一层就对应了不同方面的风格特征,这也正是其在每一层添加噪声能修改其具体风格的原因。</p>
<p>​ 基于这个特点,也就有博主提出了一种model-bleeding的方法,就是通过将两个模型的不同风格层融合得到新的模型,进而可以生成混合风格的生成图像。具体就是针对选择的生成层,将其中的参数进行融合。 <span class="math display">\[
w_n = \gamma \cdot w_1 + (1 - \gamma) \cdot w_2
\]</span> 参数融合有两种方法,一种是软融合,就是将两个模型的参数比例系数<span class="math inline">\(\gamma\)</span>进行加权融合,第二硬融合也就是<span class="math inline">\(\gamma = 1\)</span>的特殊情况,即直接把模型1中一些生成层的参数换成对应模型2中的值得到新的模型,以下的融合均采用的是硬融合的方法。这里就贴一下模型融合部分的代码如图</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/blendcode.png" alt="blendcode" style="zoom:50%;"></p>
<h5 id="模型融合的生成结果">模型融合的生成结果</h5>
<p>既然要生层浮世绘的现代现代人像,那就是希望人像的样貌接近现代,而基于渐进式生成模型的原理,整个图像的画风则近似浮世绘,而图像的风格生成主要是高分辨率层在控制,人物的形象则是在低分辨率是就基本确定。因此这里选择使用ffhq数据集训练的人脸生成模型作为基础模型,然后再将高分辨率的生成层逐步替换为浮世绘模型的参数,使得模型具有浮世绘的图像风格,方法和替换后的结构可以见图</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/shit.png" alt="shit" style="zoom:48%;"></p>
<p>​ 这里根据融合程度共新建了5个融合模型,然后对不同模型使用相同的输入即可得到风格渐变的生成结果。最终经过挑选后得到的较为可观的渐变的生成结果如图所示。这里面基本上16X16级别的融合模型就很好的实现了浮世绘风格的现代人人像的目标。而其中也有很多较为有趣的点,比如生成结果中第六列的女性,在ffhq人像模型中,其生成结果有一个抱手的动作,但当浮世绘模型逐渐替换到低分辨率的生成层时,此时人物的风格也开始被浮世绘模型所干涉,因此原本的手掌变成了扇子,真是神奇。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/best-transfer.jpeg" alt="best-transfer" style="zoom:50%;"></p>
<h4 id="一些思考">一些思考</h4>
<p>​ 融合生成结果中也有一些比较诡异的部分,比如就中等程度（32or64)的融合会出现非常惊悚的结果如图[],这里我就猜测一方面人脸模型的负责生成眼睛的中间层不能完整的工作的同时浮世绘层有没有足够的特征生成丰富的面部表情。导致中等程度的融合（32或64)面部的内容出现了确实,才有了出现比较“少儿不宜”的生成结果。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/bled32.jpeg" alt="bled32" style="zoom:33%;"></p>
<p>​ 此外还做了以浮世绘模型为基础的反向融合,效果也不是太好,我猜测原因也是由于浮世绘模型中的人脸特征不足以给出一个可以在“真实世界”里成型的人脸,导致在以真实世界作为高分辨率层的模型中人脸是扭曲的。</p>
<p><img src="/2020/02/27/gan-ukiyo-generator/figure/vertransfer.jpeg" alt="vertransfer" style="zoom: 33%;"></p>
<h4 id="reference">Reference</h4>
<ul>
<li><p><a href="https://shartoo.github.io/2019/05/12/edit-stylegan-humanface/" target="_blank" rel="noopener">使用StyleGAN训练自己的数据集</a></p></li>
<li><p><a href="https://www.justinpinkney.com/ukiyoe-dataset/" target="_blank" rel="noopener">Ukiyo-e faces数据集</a></p></li>
<li><p><a href="https://www.justinpinkney.com/stylegan-network-blending/" target="_blank" rel="noopener">模型融合方法</a></p></li>
<li><p>StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks</p></li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> CV </tag>
            
            <tag> personal tiny project </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Personal Project」用LSTM实现唐诗生成]]></title>
      <url>/2020/02/27/rnn-poem-generator/</url>
      <content type="html"><![CDATA[<p>Aaa</p>
<a id="more"></a>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> personal tiny project </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」seqGAN——将GAN与Policy Gradient带入NLP领域]]></title>
      <url>/2020/02/27/seqGAN/</url>
      <content type="html"><![CDATA[<p>前段时间使用上交开源的seqGAN重新实现了Poem Generator,这几天回头细读一下文章</p>
<a id="more"></a>
<h3 id="abstract">Abstract</h3>
<h3 id="proposed-model">Proposed Model</h3>
<p>为了方便阅读，我们先记</p>
<ul>
<li>discriminator <span class="math inline">\(D\)</span></li>
<li>generator <span class="math inline">\(G\)</span></li>
<li>vocabulary <span class="math inline">\(\gamma\)</span></li>
<li>a sequence <span class="math inline">\(Y_{1:T} = (y_1,...,y_t,...,y_T),y_t ∈ \gamma\)</span></li>
<li>current state <span class="math inline">\(s = Y_{1:t-1}\)</span>(当前的状态为之前所有输出tokens的序列)</li>
<li>next state <span class="math inline">\(s^{,} = Y_{1:t}\)</span></li>
<li>action <span class="math inline">\(a = y_t\)</span></li>
</ul>
<p>在上面的预设下，所有Text-Generation类任务，都可以抽象化为在<span class="math inline">\(t\)</span>时刻，已经生成<span class="math inline">\((y_1,..y_{t-1})\)</span>的基础上，采取什么action（如何选取<span class="math inline">\(y_t\)</span>）。</p>
<h4 id="seqgan-via-policy-gradient">SeqGAN via Policy Gradient</h4>
<p>生成器有<span class="math inline">\(G_\theta(y_t|Y_{1:t-1})\)</span> 在对抗学习的情境下，<span class="math inline">\(G\)</span> 目标函数是</p>
<p><span class="math display">\[
&lt;Empty \space Math \space Block&gt;
\]</span></p>
<p>那么<span class="math inline">\(D\)</span>是怎么计算每一步的value的呢？</p>
<p>可这样的话，Discriminator只能对一整个seq进行判别 这里就要Rollout Policy出场了</p>
<p>在前<span class="math inline">\(T-1\)</span>时刻 <span class="math display">\[
Q^{G_\theta}_{D_\phi}(a = y_t;s = Y_{1:t-1}) =
\frac{1}{N}\sum^{N}_{n=1}D_\phi(Y_{1:T}^{n})
\]</span></p>
<p>其中 <span class="math display">\[
Y_{1:T}^{n}
\]</span></p>
<p>最后的<span class="math inline">\(T\)</span>时刻，由于，序列已经生成了，就不用</p>
<p><span class="math display">\[
 Q^{G_\theta}_{D_\phi}(a = y_T;s = Y_{1:T-1}) = D_\phi(Y_{1:T})
\]</span></p>
<p>是在看完代码之后，才明白在Rollout里面也保存着一套LSTM，这就是paper中说的<span class="math inline">\(G_\beta\)</span>，</p>
<h4 id="step">Step</h4>
<p>先用MLE去预训练<span class="math inline">\(G_\theta\)</span>，再用<span class="math inline">\(G_\theta\)</span>生成的“赝品”和真实数据共同去预训练<span class="math inline">\(D_\phi\)</span></p>
<p>action value function : <span class="math inline">\(Q^{G_\theta}_{D_\phi}(s,a)\)</span></p>
<p><img src="/2020/02/27/seqGAN/figure/seqgan.png" style="zoom:30%;"></p>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> RL </tag>
            
            <tag> paper reading </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[seqGAN后续——代码阅读+重构新版PoemGenerator]]></title>
      <url>/2020/02/27/seqGAN-code/</url>
      <content type="html"><![CDATA[<p>aaa</p>
<a id="more"></a>
<p>这是上一篇paper-reading的后续。在NLG中，强化学习肯定是相当重要的一种思路，因此GAN的实现还是相当有必要的，而seqGAN可以说是的文本GAN中的baseline了（leakGAN maskGAN etc.）所以这几天半抄半改的复现了seqGAN，并在上面跑了一下全唐诗（相比之下自己以前的LSTM-Generator纯属小玩具orz）。</p>
<h2 id="code">Code</h2>
<p>在读了seqGAN的代码的基础上，我也根据自己的习惯小幅重构了一下，也方便加深理解</p>
<h3 id="build-generator">build Generator</h3>
<p>说实话我并不理解这里作者为什么要手撸一个LSTM（肯定不是不会用API XD） 我目前的理解是直接用<code>tf.nn.rnn.LSTMcell</code>的话不方便向Rollout中的<span class="math inline">\(G_\beta\)</span>传Variable。不过代码里很多tf函数的用法也还是第一次见到，学到了不少。</p>
<h4 id="unrolled-rnn-unit">Unrolled RNN unit</h4>
<p><code>start token</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用TensorArray便于读写</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">gen_o = tf.TensorArray(dtype=tf.float32, size=self.c,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">                                        dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">gen_x = tf.TensorArray(dtype=tf.int32, size=self.config.seq_maxlen,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">                                        dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#unrolled lstm </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">_, _, _, self.gen_o, self.gen_x = tf.while_loop(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    cond=<span class="keyword">lambda</span> i, _1, _2, _3, _4: i &lt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    self.sequence_length,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    body=_g_recurrence, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    loop_vars=(tf.constant(<span class="number">0</span>, dtype=tf.int32),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">self.gen_x = self.gen_x.unstack()</span></pre></td></tr></table></figure>
<p>这里的<code>tf.while_loop</code>还是第一次见到orz,详解见<a href="https://blog.csdn.net/u011509971/article/details/78805727" target="_blank" rel="noopener">这里</a>简单说一下把，<code>tf.while_loop</code>函数中<code>cond</code>和<code>body</code>分别是进行判别和执行操作的函数，他们都会接收<code>loop_vars</code>中的所有参数，所以要注意参数的位置和数量的一致</p>
<h4 id="pretrain-loss">pretrain loss</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">self.pretrain_loss = -tf.reduce_sum(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.num_emb, <span class="number">1.0</span>, <span class="number">0.0</span>) * tf.log(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">            tf.clip_by_value(tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.num_emb]), <span class="number">1e-20</span>, <span class="number">1.0</span>)</span></pre></td></tr></table></figure>
<h3 id="build-discriminator">build Discriminator</h3>
<p>discriminator的任务就是对于生成的文本输出一个介于0和1之间的分值，在本次的seqGAN中Discriminator使用了一个多层CNN用来执行分类任务</p>
<h4 id="cnn-unit">CNN unit</h4>
<p>说实话，炼丹这么久了，CNN正经一点了解都没有，真是太丢人了。 就在这里顺道学一下CNN吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">pooled_outputs = []</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> filter_size, num_filter <span class="keyword">in</span> zip(filter_sizes, num_filters):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv-maxpool-%s"</span> % filter_size):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># Convolution Layer</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        filter_shape = [filter_size, embedding_size, <span class="number">1</span>, num_filter]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_filter]), name=<span class="string">"b"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        conv = tf.nn.conv2d(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            self.embedded_chars_expanded,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">            W,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            padding=<span class="string">"VALID"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            name=<span class="string">"conv"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># Apply nonlinearity</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=<span class="string">"relu"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># Maxpooling over the outputs</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        pooled = tf.nn.max_pool(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">            h,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">            ksize=[<span class="number">1</span>, sequence_length - filter_size + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            padding=<span class="string">'VALID'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">            name=<span class="string">"pool"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        pooled_outputs.append(pooled)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Combine all the pooled features</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">num_filters_total = sum(num_filters)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">self.h_pool = tf.concat(pooled_outputs, <span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">self.h_pool_flat = tf.reshape(self.h_pool, [<span class="number">-1</span>, num_filters_total])</span></pre></td></tr></table></figure>
<p>这里的输入x是sequence已经被展开到一维的embeddings。这里的多层CNN就能将一个长度为<code>embed_size * seq_len</code>的向量降到</p>
<h4 id="highway-unit">Highway unit</h4>
<p>highway层为了加快收敛而添加的</p>
<p>关于highway，我参考了<a href="https://zhuanlan.zhihu.com/p/35019701" target="_blank" rel="noopener">这里</a>，paper原文在<a href="http://arxiv.org/abs/1505.00387" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr></table></figure>
<h3 id="roll-out">Roll out</h3>
<p><span class="math inline">\(D\)</span>和<span class="math inline">\(G\)</span>都构建完成，就要开始rollout了，Rollout在初始化时，<span class="math inline">\(G_\beta\)</span>会接受一整套预训练完成的Generator的LSTM，作为初始值。</p>
<p>在更新时，update-rate就上场了，在update时，rollout中的LSTM并不是原样接收<span class="math inline">\(G_\theta\)</span>中的值，而是部分保留，部分更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">G_bate = G_beta * (<span class="number">1</span> - updata_rate) + G_theta * update_rate</span></pre></td></tr></table></figure>
<h4 id="get-rawards">Get rawards</h4>
<p>这里可以说是seqGAN的核心function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_reward</span><span class="params">(self, sess, input_x, rollout_num, discriminator)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    rewards = []</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rollout_num):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">#D开始对生成sample中的每个token打分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> given_num <span class="keyword">in</span> range(<span class="number">1</span>, self.sequence_length ):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># t &lt; t 时 会使用G_beta生成token补全sequesnce</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            feed = &#123;self.x: input_x, self.given_num: given_num&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">            samples = sess.run(self.gen_x, feed)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            feed = &#123;discriminator.input_x: samples, discriminator.dropout_keep_prob: <span class="number">1.0</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">            ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            ypred = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> ypred_for_auc])</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                rewards.append(ypred)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">#第一轮rollout生成打分栏</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                rewards[given_num - <span class="number">1</span>] += ypred</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">#后面直接往上加</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># the last token reward</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        feed = &#123;discriminator.input_x: input_x, discriminator.dropout_keep_prob: <span class="number">1.0</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        ypred = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> ypred_for_auc])</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">            rewards.append(ypred)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">            <span class="comment"># completed sentence reward</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">            rewards[self.sequence_length - <span class="number">1</span>] += ypred</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    rewards = np.transpose(np.array(rewards)) / (<span class="number">1.0</span> * rollout_num)  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">#batch_size x seq_length</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> rewards</span></pre></td></tr></table></figure>
<h3 id="optimize">Optimize</h3>
<p>获得了reward之后，就要开始优化了</p>
<p>着重读一下这几个loss吧</p>
<h3 id="run">RUN</h3>
<p>基本模块都完成了，现在就开始GAN吧！ 先理一下训练流程</p>
<h4 id="adversial-train">Adversial Train</h4>
<p>这是一个epoch的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1</span>):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    samples = generator.generate(sess)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">#注意，一次generate生成一整个seqence个而非一个token</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    rewards = rollout.get_reward(sess, samples, <span class="number">16</span>, discriminator)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    feed = &#123;generator.x: samples, generator.rewards: rewards&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    _ = sess.run(generator.g_updates, feed_dict=feed)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> total_batch % <span class="number">5</span> == <span class="number">0</span> <span class="keyword">or</span> total_batch == TOTAL_BATCH - <span class="number">1</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    likelihood_data_loader.create_batches(eval_file)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    test_loss = target_loss(sess, target_lstm, likelihood_data_loader)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update roll-out parameters</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">rollout.update_params()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the discriminator</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    dis_data_loader.load_train_data(positive_file, negative_file)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        dis_data_loader.reset_pointer()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> xrange(dis_data_loader.num_batch):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">            x_batch, y_batch = dis_data_loader.next_batch()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">            feed = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">                discriminator.input_x: x_batch,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">                discriminator.input_y: y_batch,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">                discriminator.dropout_keep_prob: dis_dropout_keep_prob</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">            _ = sess.run(discriminator.train_op, feed)</span></pre></td></tr></table></figure>
<h3 id="orcal-lstm">Orcal LSTM</h3>
<p>说实话，刚看代码的时候，一直不明白source里的<code>target_lstm.py</code>是干嘛的，还以为是作者写的LSTM的模版，最后重读了一边论文，才发现这部分的作用。原文在实验的时候引入了orcale-model，说白了就是他们有一个在目标任务上预训练的相当成熟的LSTM可以直接生成正样本相比于从零开始,模型收敛会快上不少，而我在跑Chinese-Poem的时候，这部分肯定就没有了。</p>
<h3 id="others">others</h3>
<h4 id="data-until">data until</h4>
<p>这次用的数据是ChinesePoem<a href="http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/rnnpg_data_emnlp-2014.tar.bz2" target="_blank" rel="noopener">下载</a> 有28W首。 数据以40为单位做了切分，并做了mask</p>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> RL </tag>
            
            <tag> paper reading </tag>
            
            <tag> personal tiny project </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[兜兜转转又一年]]></title>
      <url>/2020/02/19/twenty/</url>
      <content type="html"><![CDATA[<p>  敲字儿这会儿，已经过了零点了，20岁了</p>
<a id="more"></a>
<p>  对于所谓“成长”的记忆大都来自书本和电影，印象最深的就是小时候在电视上看《少林寺》，李连杰在一个过场里冬练三九夏练三伏，日日精进，绝圣弃智，三分钟bgm过后，他就蜕变成一个健壮的武僧了。</p>
<p>  但回到自己身上，或许是因为生活太安逸，对所谓成年，所谓长大，从未真正的体验过，空有道理而无感受。这是值得警惕的，这一年的经历让你开始明白，缺乏感受的生活必将浑浑噩噩。<strong>希望这些下面这些矫情的话可以帮你在未来的某一天唤起斗志</strong>。</p>
<p>  说起今年最大的遗憾，是上半年因为自己的懈怠，错失了一个可能相当宝贵的机会，无法与那群优秀的同龄人们一起同行共事，再回想起去年这个时候为了它而做出的努力，不得不说，挫败感还是相当重的。不过既然骰子已掷出，没资格后悔。</p>
<p>  今年或者说这大学这两年，真的没有什么值得骄傲的东西，十分害怕自己或许冥冥之中已经接受了某种平庸。。。</p>
<p>  在对自己的认识上，你是相对早慧的，你很早就明白那些让自己焦虑的因素是什么，来自何处，并且很早就学会了为自己在彼岸编织“解脱”之道。可惜的是，由于懒惰与自满，自己不断的食言，以至于你需要一次又一次为自己编织新的“未来”，来满足一种脆弱的优越。在2019年，这种循环已经走到了边缘，一次次的挫败已经开始触及你最宝贵的东西——真诚与自信。</p>
<p>  那些你曾经乐此不疲的“豪言壮志”，已经成为最值得警惕的敌人，<strong>这种虚假的达成感在真正的减损你日常生活中实践的效率</strong>，它让你无法分辨真正的“好”与廉价的“爽”，在虚假的自洽中变得自满，遮蔽真实的感受。</p>
<p>  用一个牵强的比喻，沉迷“抖音”没什么值得警惕的，因为大家都处在很自知的“玩物丧志”中，但凡才及中人，都有能力抽身而出。相比之下，“b站”“知乎”之流才是糟糕的，它让人沉溺在虚假的达成感中而不自知，反倒开始自欺欺人的辩解，力图争夺虚幻的优越而忘记真正的实践。（好吧我承认这个比喻实在有够奇怪，明白意思就好）</p>
<p>  在以后的日子里，一定要记住，不凡绝不可能在一节慷慨的进行曲中开开心心的达成。再热爱的事也要在枯燥的实践中才能达成，“好”与“爽”是不可能同时得到。</p>
<p>  另一方面，不要再妄自菲薄，很多与他人无意义的比较只会减损斗志，<strong>少说多做</strong>才会带来好的感受，要用感受而非思辨去对抗那种自卑与懒惰。</p>
<p>  好在你终究还是抱有着最后一丝自信力，那股劲儿还在。下半年拾起了长跑，坚持了几个月没有懈怠，是值得开心的。现在的每月130公里，确实是最初不敢想的，实属不易，等这段日子过去，要继续精进，不要放下！</p>
<p>  大道理讲完了，再说说实际的期许吧。在学业上，如果真的对NLP有热忱，一定要分秒必争（不出意外明年这个时候就要准备考研了，ZJU等我）本科也就剩这一年可以专心学习了，不要害怕自己资源或者资质不够，属于自己的道路只能是独自摸索出来的，有很多未知的好运真的只有在实践的路上才能遇到。</p>
<pre><code>“怕什么真理无穷，进一寸有一寸的欢喜”</code></pre>
<p>（胡适这句话虽然被用烂了，但不得不说确实是催人奋进的）</p>
<p>  在运动上，关于跑步的两个小目标，校运会10km拿名次和一场马拉松总是要做到的（提前祈祷杭马或者宁马能中签），运动时的那种专注与真诚是相当很珍贵的。</p>
<p>  最后就是对生活的一些期许了，这一年和身边人的关系大都若即若离，接下来的一年希望不论是技术上还是观念上都可以遇到值得相处的人。此外在人们越来越抗拒真诚的社会中，亲密关系中的袒露变得越来越不可或缺，就像坨式所讲「“人们不能用禁闭自己的邻人来确认自己神志健全。”」只有在与人真诚的分享中我们才能真正的找到自己。一言蔽之如果身边真的有心仪的姑娘，别再怂了。</p>
<p>  要说的差不多了，希望新的一年可以不负所言，用实践的感受来对抗虚无，用真实的袒露来克服孤独，“中道”必然蕴藏在实践和感受当中。好了不早了，就这样吧，新的一岁，祝武运昌隆！</p>
]]></content>
      
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[sum的强化学习(RL)笔记]]></title>
      <url>/2020/01/28/RL1/</url>
      <content type="html"><![CDATA[<p>前一段时间开始seqGAN第一次接触到RL，正好寒假时间富裕，补一些强化学习的基础 <a id="more"></a></p>
<h2 id="key-concepts">Key Concepts</h2>
<p>在正式开始RL前,我们先要对强化学习的一些基本概念和目标有所了解，其中主要的，如果对基本概念有不理解可以看<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" target="_blank" rel="noopener">这里</a></p>
<ul>
<li><span class="math inline">\(s \in \mathcal{S}\)</span> <strong>states</strong>；<span class="math inline">\(a \in \mathcal{A}\)</span> <strong>actions</strong>；<span class="math inline">\(r \in \mathcal{R}\)</span> <strong>rewards</strong> ；<span class="math inline">\(\pi\)</span> <strong>Policy</strong></li>
<li><span class="math inline">\(P(s&#39;, r \vert s, a)\)</span> <strong>state-transition</strong>在状态<span class="math inline">\(s\)</span>下采取行动<span class="math inline">\(a\)</span>后转移到状态<span class="math inline">\(s&#39;\)</span>的概率<br>
<span class="math display">\[P_{ss&#39;}^a = P(s&#39; \vert s, a)  = \mathbb{P} [S_{t+1} = s&#39; \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s&#39;, r \vert s, a)\]</span></li>
<li><span class="math inline">\(R\)</span> <strong>Reward Function</strong> <span class="math display">\[R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s&#39; \in \mathcal{S}} P(s&#39;, r \vert s, a)\]</span></li>
<li><span class="math inline">\(G_t\)</span> <strong>Return</strong> <span class="math display">\[G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]</span></li>
<li><p><span class="math inline">\(\gamma\)</span> <strong>discounting factor</strong> <span class="math inline">\(\gamma \in [0, 1]\)</span></p></li>
<li><span class="math inline">\(Q^\pi(s, a)\)</span> <strong>Action Value</strong>(“Q-value”) 采取策略<span class="math inline">\(\pi\)</span>时，状态<span class="math inline">\(s\)</span>下，执行<span class="math inline">\(a\)</span>的累计回报 <span class="math display">\[Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a]\]</span></li>
<li><span class="math inline">\(V^\pi(s)\)</span> <strong>State Value</strong>采取策略<span class="math inline">\(\pi\)</span>时,状态<span class="math inline">\(s\)</span>下的累计回报;也可写作<span class="math inline">\(Q(s)\)</span> <span class="math display">\[V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s]= \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)\]</span></li>
<li><p><span class="math inline">\(A(s, a)\)</span> <strong>Advantage Function</strong>,（“A-value”）,定义为<span class="math inline">\(A(s, a) = Q(s, a) - V(s)\)</span></p></li>
</ul>
<h3 id="mdp">MDP</h3>
<p>RL的几乎所有任务都可抽象成马尔可夫过程（Markov Decision Process, MDP）</p>
<p><span class="math display">\[
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
\]</span></p>
<p>看似上式中的<span class="math inline">\(s_{t+1}\)</span>是一个套娃，是由之前所有时刻的状态得到的，但这里有一个关键点，一旦<span class="math inline">\(s_t\)</span>已知，便可由<span class="math inline">\(s_t\)</span>独立推出<span class="math inline">\(s_{t+1}\)</span>，之前的状态都可以扔掉。因此Text Generation这样的NLP任务并不具备马尔可夫性质(我猜)。</p>
<p>马尔可夫决策过程由<span class="math inline">\((S,A,P,R,\gamma)\)</span>组成，<span class="math inline">\(S\)</span>为有限的状态集,<span class="math inline">\(A\)</span> 为有限的动作集, <span class="math inline">\(P\)</span> 为状态转移概率, <span class="math inline">\(R\)</span>为回报函数, <span class="math inline">\(\gamma\)</span> 为折扣因子，用来计算累积回报（Return）</p>
<p>下面这个式子就是马尔可夫过程的核心了。值得注意的是状态转移的过程中，假定我们在状态<span class="math inline">\(s\)</span>这个节点上，我们采取行动<span class="math inline">\(a\)</span>，状态转移为<span class="math inline">\(s&#39;\)</span>的概率不是0或1，而是一个随机变量。 <span class="math display">\[
p_{s\hat s}^a=p(\hat s|s,a)=p(S_{t+1}=\hat s| S_t=s,A_t=a)
\]</span></p>
<h4 id="stationary-distribution">Stationary Distribution</h4>
<p>稳态分布（Stationary Distribution）在后面的policy gradient会用到，他是这么定义的：<strong>如果一个非周期马氏链具有概率转移矩阵 P，且它的任何两个状态都是连通的，则 <span class="math inline">\(\lim\limits_{n\to\infty}P_{ij}^n\)</span> 存在且与<span class="math inline">\(i\)</span>无关</strong>，用人话说就是服从<span class="math inline">\(\pi\)</span>马尔可夫链不同的初始概率分布经过足够长的时间都可可以收敛到一个相同的概率分布，这就是稳态分布<span class="math inline">\(d_\pi\)</span>。</p>
<h4 id="bellman-equation">Bellman Equation</h4>
<p>一言蔽之，就是对所有的状态转移与回报建立模型，表示出在马尔可夫链中的的任意<span class="math inline">\(s\)</span>点时的<span class="math inline">\(V(s)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
V(s) &amp;= \mathbb{E}[G_t \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
\]</span> <img src="/2020/01/28/RL1/bellman_equation.png" alt="bellman"></p>
<h4 id="优化目标">优化目标</h4>
<p>在这类MDP问题中，我们优化的目标无非是找到policy以获得最大的Return，计作<span class="math inline">\(V_*(s)\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
V_*(s) &amp;= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a V_*(s&#39;) \\
V_*(s) &amp;= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a V_*(s&#39;) \big) \\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a \max_{a&#39; \in \mathcal{A}} Q_*(s&#39;, a&#39;)
\end{aligned}
\]</span></p>
<figure>
<img src="/2020/01/28/RL1/v2-604dcc4d56bfdfe410886f99520a5bdb_hd.jpg" alt="DP"><figcaption>DP</figcaption>
</figure>
<p>假设如上图，我们对所处的环境门儿清，明确的知道任意状态下行动的回报时，这就成了一个DP问题，套贝尔曼方程，求<span class="math inline">\(V_*(s)\)</span>就好了。但这么理想但并不是今天讨论的重点，我们知道在真实环境下，对客观世界是永远不可能建立那么理想的状态转移图，只能是在一次次的观测与重复中“揣测”真实世界，甚至连所有行动的reward也是我们自行评估的，而这就需要强化学习登场了。</p>
<h3 id="蒙特卡洛方法">蒙特卡洛方法</h3>
<p>是在前段时间的seqGAN第一次接触到MC-method，当时感觉蛮高级的，也没有深入去了解。但就现在大致了解的MC，感觉其实就是是统计学游戏。无异于通过无数次的盲人摸象，关于MCMC，以后会深入的学习。</p>
<h2 id="从一根杠杆开始">从一根杠杆开始</h2>
<p>google不愧人类之光(暴论)！openAI的gym库里面提供了很多的虚拟环境，用于强化学习的训练和开发，而<a href="http://gym.openai.com/docs/" target="_blank" rel="noopener">CartPole</a>绝对可说是RL界的HelloWorld。它的规则很简单，在一个存在重力的环境下，智能体（agent）在每一时刻做出左移或者右移的action，保持杠杆倾斜不超过15度。我们的目标是让杠杆存活尽可能长的时间。</p>
<h3 id="policy-gradient">Policy Gradient</h3>
<h4 id="abstract">Abstract</h4>
<p>强化学习旨在帮助我们找到能获得最大回报的策略，而作为强化学习的一种重要方法，policy gradient直接对策略进行建模与优化，它的reward function被定义为 <span class="math display">\[
J(\theta)
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s)
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
\]</span></p>
<p>好吧我承认，上面的话主要的目的在于提升本文逼格，对初学者理解policy gradient并没有太大帮助，下面我来试着用自己的理解来谈谈policy gradient</p>
<h4 id="我来给翻译翻译">我来给翻译翻译</h4>
<p>在传统的监督学习中，假使我们有足够多且理想的“正确答案”作为label，那么直接将交叉熵的结果进行反传，就可以拟合出足够好的模型。但在很多决策问题上，我们根本无法得到什么有效的label，就像在CartPole游戏中，我们的训练是没有任何数据集作为参考的。</p>
<p>其实可以把这类把任务看成像一站到底一样的知识竞赛，答错一定数量的题目即算出局。在我看来，传统的的监督学习就是搜集能到所有有题库和答案的选手，他们可以一遍一遍的背题库，最后的无限接近全对。而policy gradient下的的选手则是一次又一次的参加竞赛，没有答案可供参考，游戏结束后他获得的唯一信息也只是知道自己活了多少轮。</p>
<p>现在我们再回头，看看开头的的式子，似乎也就有了头绪，正如齐名，policy gradient就是对全局的policy进行建模与优化，具体在CartPole游戏里，它也是在一局游戏结束，才进行一次“复盘”，而它的目标函数也是由两部分相乘得到，第一部分是最大似然的交叉熵，第二部分是一局中所有时刻的Return。</p>
<p>对于第一部分，我们自不会陌生，它评估的是在特定state下，随机变量<span class="math inline">\(\pi(s)\)</span>和实际action的偏差程度，在这里体现的就是模型对自己选择的确信程度，在只考虑这部分目标函数的情况下，优化目标函数，即是提供了一个正反馈，这个模型越来越相信自己在最初状态随机作出的决策（这里说的不是很严谨，只是我自己的理解），越来越“刚愎自用”，这自然是不行的。</p>
<p>这里就需要引入Return了，在我看来，它相当于在复盘时给每步action打分，它相当于告诉了模型，，到底有多少可取的部分。</p>
<p>根据有针对性的reward进行反传，很快就可以拟合出不错的策略。听起来很理想，但这就引出了下一个问题，也就是policy gradient的核心，到底如何得到reward？</p>
<p>还是以CartPole为例，至少在在<code>CartPoleV1</code>这个环境里，对于单步reward的定义非常简单，游戏中agent每次action的reward都是1，听起来这个单步的reward的定义好像十分草率。不过也是十分巧妙的，我们再回想</p>
<h4 id="babymath可跳过">babyMath（可跳过）</h4>
<p>接下来，我们来推导一下policy gradient目标函数的梯度，会有一些babyMath，不过也只是符号比较唬人，并没有劝退部分（确信），当然也可以跳过或者直接去看<a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">原文</a></p>
<p>现在，我们重新捡回reward function那个让人嫌弃的定义式，好好看看这个定义式是怎么来的，</p>
<p><span class="math display">\[
J(\theta)
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s)
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\
&amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)
\end{aligned}
\]</span></p>
<p>我们知道，在游戏的每一时刻，我们都有状态<span class="math inline">\(s_t\)</span>,此时的reward function即为<span class="math inline">\(V^\pi(s_t)\)</span>，,现在我们开始对<span class="math inline">\(\nabla_\theta V^\pi(s)\)</span>进行变换,接下来是喜闻乐见的微分环节 <span class="math display">\[
\begin{aligned}
&amp; \nabla_\theta V^\pi(s) \\
=&amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp; \scriptstyle{\text{根据全概率公式}}\\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) &amp; \scriptstyle{\text{乘法法则}} \\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s&#39;, r} P(s&#39;,r \vert s,a)(r + V^\pi(s&#39;))} \Big) &amp; \scriptstyle{\text{; Extend } Q^\pi \text{ with future state value.}} \\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s&#39;, r} P(s&#39;,r \vert s,a) \nabla_\theta V^\pi(s&#39;)} \Big) &amp; \scriptstyle{P(s&#39;,r \vert s,a) \text{ or } r \text{ is not a func of }\theta}\\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s&#39;} P(s&#39; \vert s,a) \nabla_\theta V^\pi(s&#39;)} \Big) &amp; \scriptstyle{\text{; Because }  P(s&#39; \vert s, a) = \sum_r P(s&#39;, r \vert s, a)}
\end{aligned}
\]</span> 从红色部分可以看出，上式是个套娃 为了理解reward，需要理解agent的状态转移。 <span class="math display">\[
s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s&#39; \xrightarrow[]{a \sim \pi_\theta(.\vert s&#39;)} s&#39;&#39; \xrightarrow[]{a \sim \pi_\theta(.\vert s&#39;&#39;)} \dots
\]</span></p>
<p>假设一局游戏可以抽象化为agent采取<span class="math inline">\(\pi_\theta\)</span>，经过<span class="math inline">\(k\)</span>步，从状态<span class="math inline">\(s\)</span>转移到状态<span class="math inline">\(x\)</span>,计作<span class="math inline">\(\rho^\pi(s \to x, k)\)</span></p>
<p><span class="math display">\[
\rho^\pi(s \to x, k+1) = \sum_{s&#39;} \rho^\pi(s \to s&#39;, k) \rho^\pi(s&#39; \to x, 1)
\]</span></p>
<ul>
<li><span class="math inline">\(k=0\)</span>时，相当于agent什么都没做，自然有：<span class="math inline">\(\rho^\pi(s \to s, k=0) = 1\)</span></li>
<li><span class="math inline">\(k=1\)</span>时，根据贝叶斯公式：<span class="math inline">\(\rho^\pi(s \to s&#39;, k=1) = \sum_a \pi_\theta(a \vert s) P(s&#39; \vert s, a)\)</span></li>
<li>推广到普遍情况，套娃出现了：<span class="math inline">\(\rho^\pi(s \to x, k+1) = \sum_{s&#39;} \rho^\pi(s \to s&#39;, k) \rho^\pi(s&#39; \to x, 1)\)</span></li>
</ul>
<p>现在我们记<span class="math inline">\(\phi(s) = \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)\)</span> 继续化简<span class="math inline">\(\nabla_\theta V^\pi(s)\)</span> <span class="math display">\[
\begin{aligned}
=&amp; \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s&#39;} P(s&#39; \vert s,a) \color{red}{\nabla_\theta V^\pi(s&#39;)}\\
=&amp; \phi(s) + \sum_{s&#39;} \sum_a \pi_\theta(a \vert s) P(s&#39; \vert s,a) \color{red}{\nabla_\theta V^\pi(s&#39;)} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \color{red}{\nabla_\theta V^\pi(s&#39;)} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \color{red}{[ \phi(s&#39;) + \sum_{s&#39;&#39;} \rho^\pi(s&#39; \to s&#39;&#39;, 1) \nabla_\theta V^\pi(s&#39;&#39;)]} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \phi(s&#39;) + \sum_{s&#39;&#39;} \rho^\pi(s \to s&#39;&#39;, 2)\color{red}{\nabla_\theta V^\pi(s&#39;&#39;)} \scriptstyle{\text{ ; Consider }s&#39;\text{ as the middle point for }s \to s&#39;&#39;}\\
=&amp; \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)
\end{aligned}
\]</span></p>
<p>这样我们就得到了<span class="math inline">\(\nabla_\theta V^\pi(s)\)</span> <span class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta V^\pi(s_0) &amp; \scriptstyle{\text{; Starting from a random state } s_0} \\
&amp;= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &amp;\scriptstyle{\text{; Let }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\
&amp;= \sum_{s}\eta(s) \phi(s) &amp; \\
&amp;= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp; \scriptstyle{\text{;  } \eta(s), s\in\mathcal{S} \text{ to be a probability distribution.}}\\
&amp;= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)
&amp; \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ is stationary distribution.}}\\
&amp;= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &amp;\\
&amp;= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] &amp; \scriptstyle{\text{; Because } (\ln x)&#39; = 1/x}
\end{aligned}
\]</span></p>
<p>现在我们就得到了目标函数的梯度，它与所处的特定状态<span class="math inline">\(s\)</span>无关</p>
<p><span class="math display">\[
\nabla_\theta J(\theta)  = \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)]
\]</span></p>
<p>当然了，对应，它也有很多种变体，如下图所示</p>
<figure>
<img src="/2020/01/28/RL1/general_form_policy_gradient.png" alt="mathods"><figcaption>mathods</figcaption>
</figure>
<h3 id="code">Code</h3>
<p>前边水了这么多，可能还是有些云里雾里，想要直观的理解，还是得上代码</p>
<h4 id="g_t"><span class="math inline">\(G_t\)</span></h4>
<p>获得<span class="math inline">\(\nabla_\theta V^\pi(s)\)</span>并归一化</p>
<p>这里采取了蒙特卡洛方法来计算reward</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_discount_and_norm_reward</span><span class="params">(self)</span>:</span> <span class="comment"># reward decay and normalize</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    discount_ep_rs = np.zeros_like(self.ep_rs)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    running_add = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(<span class="number">0</span>, len(self.ep_rs))):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        running_add = running_add * self.gamma + self.ep_rs[t]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">#蒙特卡洛方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        discount_ep_rs[t] = running_add</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="comment"># normalize episode rewards</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    discount_ep_rs -= np.mean(discount_ep_rs)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    discount_ep_rs /= np.std(discount_ep_rs)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> discount_ep_rs</span></pre></td></tr></table></figure>
<h4 id="object-function">Object Function</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">self.all_act_prob = tf.nn.softmax(all_act, name=<span class="string">'act_prob'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="comment"># to maximize total reward log p * R = minimize - log_p *R</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    self.loss = tf.reduce_mean((neg_log_prob * self.tf_vt))</span></pre></td></tr></table></figure>
<h3 id="reference">Reference</h3>
<p><a href="https://zhuanlan.zhihu.com/p/25498081" target="_blank" rel="noopener">强化学习入门 第一讲 MDP</a></p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-4-gym/" target="_blank" rel="noopener">OpenAI gym 环境库 by莫烦</a></p>
<p><a href="https://tobiaslee.top/2018/03/06/Reinforcement-Learning1/" target="_blank" rel="noopener">Policy Graident 从数学到实现</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25743759" target="_blank" rel="noopener">强化学习之蒙特卡罗方法</a></p>
<p><a href="http://www.foolweel.com/2019/05/02/rl-basic/" target="_blank" rel="noopener">强化学习概述</a></p>
<p><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" target="_blank" rel="noopener">A (Long) Peek into Reinforcement Learning</a></p>
<p><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Policy Gradient Algorithms</a></p>
<p><a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/" target="_blank" rel="noopener">Markov Chain Monte Carlo Without all the Bullshit</a></p>
]]></content>
      
        
        <tags>
            
            <tag> RL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谈谈我们的“异化”]]></title>
      <url>/2019/12/05/suibi1/</url>
      <content type="html"><![CDATA[<p>  些这篇文章的是前段时间的毛概大作业，虽然不长，但也费了不少心力。不得不说，写作过程还是相当痛苦的，不过也难得可以以这种自己“瞧得起”的方式表达真情实感。所以发在博客上，希望以后可以继续写作这个好习惯吧。</p>
<a id="more"></a>
<h3 id="这个时代的异化">这个时代的异化</h3>
<p>  异化，代表着人对自己的劳动与外部世界丧失控制，甚至反被操纵的过程，在马克思的笔下，它是“人的受难”，而在现今的网络空间中，我们的情绪与表达也在越来越多的呈现着这样的异化。</p>
<h4 id="清醒的哲人王们">清醒的“哲人王”们</h4>
<p>  格雷塔·桑伯格，瑞典的环保少女，因她极端反现代化的环保诉求与乖张的事迹而为人所知。2018年，15岁的她每个周五都会独自在瑞典国会前举着“为气候变化而罢课”抗议静坐。并参加了今年九月的联合国气候峰会，并因演讲中那张表情夸张的“How Dear You”而红遍中国的社交网络(这里的“红”纯粹是指其激发的情绪之剧烈)。活脱一个完美的“妄人”范式。与“疯姑娘”一同被推到风口的是，是“普京一语戳破西方环保阴谋”，“中国的绿化成就震撼世界，为英雄们点赞”。。。 「说来戏谑，每当我们被西方白左气到时，总是要抬出那些“最可爱的人们”给我们撑腰」</p>
<p>  说老实话，让我们用自己的直觉判断，也大概可以明白这位“疯姑娘”的诉求并不现实，许多乖张的事迹也确实逃不开炒作的嫌疑。可“不实际”，“不纯粹”难道就真的能作为我们情绪如此高亢的理由吗?如果“非蠢即坏”的咒骂真的源于我们的道德自觉，那么在网络上，我们对大屠杀，纳粹，红色高棉，又何曾有过如此纯粹的愤怒呢? 「主观的认为，在当今的网络中不管是大屠杀的恶，还是雷锋式的善都沦为了虚无的，关于善恶的“道理”;而网络中的高亢表达，根本上源于一种“恶感”」</p>
<p>  在这里，人人都是了不起的智者，一眼就戳穿了反智的本质与西方的阴谋，用尽工整精致的语言，向人们证明着“我什么都不做”比“反智”高明多了，再顺带对那些真正真诚的实践者施以崇高的敬意。熟练的运用着自己的理性，歌颂着“伟大”，唾弃着“虚伪”，何其高明的“哲人王”们!</p>
<p>  如果我说这么多只是为了讽刺所谓“乌合之众”，那就太过流于表面了，我想问的是，我们的愤怒与崇敬，是不是都有些太过轻巧了?在我看来这不仅仅是对于“妄人”的讽刺，更是对于“偶像”的羞辱。试问诸位，既然你们对那些投身环保的实践者们如此崇敬，请问，你们能拿出那位“疯姑娘”十分之一的热忱去做过哪怕一次你们认为“不虚伪”“有意义”的环保?我倒不妨直接说开了吧，你们口中的“崇高”“务实”的英雄们，不过是你们拿来搬弄是非的工具罢了。我们都特别明白行动和言辞的界限，从不轻易僭越到无能为力的境地。</p>
<pre><code>叶公子高好龙，钩以写龙，凿以写龙，屋室雕文以写龙。
于是天龙闻而下之，叶公见之，弃而还走，失其魂魄，五色无主。
是叶公非好龙也，好夫似龙而非龙者也。</code></pre>
<p>  在我看来，这种过分“冷静”的情绪实际上源于一种实践的不可欲，现代社会无限体量与联通，让价值的丧失成了一种必然代价，在我们父辈或更早的时候，一个人在其所在的宗族国企大院这样的共同体中，你只需要在这个共同体或者大院里做到有一技以傍身，你就能获得充分的”价值感”，而在当今这个“无限”世界中的，你所做的一切，只要打开网络，看到那些“神迹”，作为一个孤立的个体，还怎能真正的相信自己实践的价值「可能表达的有些极端，但确实有这样的倾向」。于是，在人们的言论与想象之中，个体的能量只剩零和无穷大，除非您能绝圣弃智，行出“神迹”，任我们顶礼膜拜。不然就请闭嘴。什么，你告诉我这也叫成就?五十步笑百步罢了，指不定背后还有什么不可告人的阴谋!「或许是一种语言的遮蔽，在实践的层面上，“五十步”怎么就不能“笑百步”了?」</p>
<p>  从人格上讲，“妄人”“英雄”于我们同样遥远。可我们羞于面对这种平庸，痛恨一切不能一蹴而就的改变，我们只能指望着几句精致的讽刺与假惺惺的热爱，证明自己真的与英雄们“心有戚戚焉”，对着光屏艰难着维护者一个“自洽”的我。激昂的言辞昭示的是无根之人的惶恐。以及对于独自实践的恐惧。</p>
<h4 id="庶民的胜利">庶民的胜利</h4>
<p>  今年七月后，一个不那么善意的称呼出现在了网络上--“废青”，意指那些在香港“反送中”运动中参与“暴乱”的香港年轻人。而这个称呼也是用来讽刺他们在“自以为正确”的“暴乱”中诸多幼稚与滑稽的行为。</p>
<p>  由于可能涉及一些政治敏感话题，我觉得在这里有必要申明一下，现在社会上所有发生的事，在网络这个“拟像”上激起的涟漪，从来都与事实真相无关。网络中普遍存在的“表达异化”，都是蕴藏在浪潮中不断共振的情绪而非事实的道理之中。因此我无意对所提及的事件做任何事实层面的判断，我所在乎的是“第四面墙”后观众席上的声音。在撇清责任后，让我们再次回到这场“时代革命”的浪潮中。从七月中旬，无数的浪潮一波又一波的掀起，“帝吧百万出征”“阿中女孩为国护旗”。。。无数年轻人用着自己的方式“捍卫”着的价值。</p>
<p>  在这次“全民爱国”的主阵地之一，新浪微博的“#中国”超话下，鲁迅五四时期《新青年》上的一段文章</p>
<pre><code>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。
能做事的做事，能发声的发声。
有一分热，发一分光。
就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。
此后如竟没有炬火，我便是唯一的光。</code></pre>
<p>再次被广泛的引用转发，初听起来极富感情，仔细一想，越觉得不对。难道，这些在网路上“为国护旗，挺港警”的青年们，真的把自己带入到了那些五四学生的视角了吗?</p>
<p>  在过去那些风云激荡的年代，我们抗议，写作,斗争，用着自己的热血与理想，表达着自己真实的诉求。同样，这一切“真”的行动，从来都意味着困难，风险，与代价。(我没有任何号召大家去模仿的意思，只是想说，真正的表达与捍卫，从来都是伴着付出与冒险的)</p>
<p>  而时至今日，我不妨直说，在这场斗宏大运动中的每个青年，他们都心知肚明，自己胜券在握，他们不论说出如何过激的言语，都不用承担任何真实的代价，不论是武力的对抗还是舆论的斗争，“阿中”的胜利都只是时间问题。他们在这场根本不会波及自己真实生活的战场上肆意“杀敌”。</p>
<p>  他们的慷慨陈词，与其说实在发自内心的捍卫立场，不如说是在胜券在握的的关头，毫无风险，毫无代价的享受着胜利的喜悦!如果说那些不辨菽麦走上街头的勇武派是“废青”，那么在网路上嗜胜的青年们，绝对是“强青”了!换一种极端的讲法，与其说他们是爱国，爱某些价值，不如说他们是爱“赢”罢了。</p>
<p>  热血方刚的青年们，享受着毫无代价，毫无风险的“勇气”与“热情”，所带来的快感。可这终归是“假勇气”，“假热情”。我想问，在洞穴中沉溺于烛光倒影的囚徒们的，就算解开镣铐，还有没有胆量去直面阳光与这真实的世界呢?</p>
<h4 id="代庖之乐">“代庖”之乐</h4>
<p>  如果说这些“假捍卫”“假勇气”的只是一种不自知的异化，那这场浪潮中的另一幅面向，则呈现出来一种更深的异化与懦弱。</p>
<p>  大概是八月中旬一组“深圳警方千人实战演习”的短视频刷爆了网络，最令我惊讶的是在评论中多数都是类似“年度大戏，即将上演”，“结局到来，敬请期待!”这类具有“幽默”与期待的讽刺。(毫无疑问大家都明白着这隐喻着什么)如果说帝吧出征与香港青年对骂还是算是自发行动去捍卫自己的立场的一场自维的“战斗”，那大家对“深圳警方演习”“驻港部队换防”这样的拍手称快，则纯粹是一种对于“代庖”的享受，我甚至可以极为武断的说，如果这些表述中有一个主导的情绪，那一定是“胜利的喜悦”。 「这样糟糕的情绪，自然与不断的对立与极端化报道脱不开干系，青年们想象中的敌人让早已非人化，不过可悲的这已不是什么新鲜事了」</p>
<p>  伴随这种“代庖”，还有一个更糟糕的面向，那就是在网络各种场合“举报”这种表达形式的蔚然成风。在微博上许多明星的“反黑组”「意指粉丝中为了维护明星声誉而在网络上巡查监控负面言论的自组织形式」，每天都会定时贴出数十甚至上百的账号信息，用以让粉丝团体精确举报，通过官方封号来消除负面评价。可见我们对于这种“高效”的斗争形式适应的相当之快。</p>
<p>  可以说，这种在网络世界上对于“代庖”的渴望，是我们面对如此庞大的现代社会与国家机器时，最舒适的获得“自洽”的方式。可是，“代庖”从来不是没有代价的，他的代价，就是纯粹的交托，纯粹的异化，放弃一切应然的执念，相信自己外部的一切都是“State of The Art”(当前最好的结果)。才能心安理得。</p>
<p>  最后我只想说，这种享受“代庖”的狂欢，已不仅仅是的异化表达，而是这个庞大现代社会所异化的虚弱之人唯一的表达罢了!</p>
<h3 id="写在最后">写在最后</h3>
<p>  其实每当有这样“文以载道”的机会，我都会燃起不小的热情，想去说点什么，可当我真正的拿起笔，很多原来引以为傲的“灼见”，落到纸上，都只剩一些搬弄是非的文字游戏，读完尽是些言之无物的空谈。这种挫败也带给了我一个新的视角，是不是在网络时代，我们的“言之无物”成为了一种必然。面对那么多富有煽动力的媒介内容源源不断的涌来，我们的观点变得毫无意义，我们只能说“好”与“坏”，或是运用“精致的文字游戏”去捍卫别人预设好的立场。</p>
<p>  经历了一天极为痛苦的写作，也让我意识到了，写作是一个内在蕴含反思的过程，真正落到纸上的文字，才是经得起反思，蕴含着真实表达的。所以最后我想说，对这种表达的异化。我们并不是完全无力的，但终是要付出一些困难的。要摒弃这种异化，就要让我们的表达不再是，“搬弄是非”，承载着“假勇气”的“文字游戏”，少争吵，多写作。</p>
<p>  文章不长，描述很多的现象其实也充满了我主观的臆想，不过我坚信，在这个“人人都是知识分子”的时代，真的情感远比精致的道理来的可贵，一如深刻的视角比最后真相更能让人反思，指引我们去真正的实践!</p>
]]></content>
      
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」AlphaGo + AlphaGoZero]]></title>
      <url>/2019/11/30/AlphaGo/</url>
      <content type="html"><![CDATA[<p><a href></a></p>
]]></content>
      
        
        <tags>
            
            <tag> RL </tag>
            
            <tag> paper reading </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」RCNN family —— 基于Region Proposal的目标检测模型]]></title>
      <url>/2019/11/30/rcnn/</url>
      <content type="html"><![CDATA[<p>R-CNN的横空出世标志着“目标检测”被纳入了深度学习的板块之中,RCNN家族是从传统的基于手工特征的目标检测框架逐步迭代而来的,因而其模型也相对复杂,与于之后纯粹源于深度学习的YOLO系列模型形成了鲜明对比.</p>
<a id="more"></a>
<p>本文将从RCNN入手,记录基于Region Proposal的目标检测模型的基本结构, 并补充其后续版本的改进之处(Fast R-CNN .e.tc.).</p>
<h4 id="od任务的基本问题-localization-classification">OD任务的基本问题: localization + classification</h4>
<p>借用知乎上一个博主的总结,OD要解决的问题就是“物体在哪里以及是什么”的流程问题.而其中“是什么?”其实是CV中典型的classification问题,已经有很多成熟的模型可直接调用.因而解决“物体在哪里”的localization任务,就成了OD任务的重点.为了方便即后续讨论,对于localization问题,我们可以记</p>
<blockquote>
<p>输入：图片</p>
<p>输出：方框在图片中的位置（x,y,w,h）</p>
<p>label: 物体在图片中的实际位置（x,y,w,h）</p>
<p>评估方法：检测评价函数 intersection-over-union(IOU,也就是俗称的召回率)</p>
</blockquote>
<p>简而言之,localization就是输出物体坐标(根据坐标即可得到物体的bonding box),并最大化IOU,即尽量使预测bonding box与实际物体框重合.对于localization,有两种处理思路</p>
<h5 id="思路一将localization看作回归问题">思路一:将localization看作回归问题</h5>
<p>当将localization看作回归问题,即使用label数据回归预测（x,y,w,h）四个参数,得到bondingbox.这实际上延续了CV中解决classification问题的思路,只需改变一下输出层的结构(将输出层由类别改为四个坐标),即可使用经过预训练的CNN模型(Vgg,AvexNet e.t.c)来完成localization.</p>
<p><img src="https://images2015.cnblogs.com/blog/1093303/201705/1093303-20170504112723757-880743532.png"></p>
<p>直觉上讲,我会觉得regression的思路与机器学习的模型更为契合,不幸的是RCNN的作者提到回归法在目标检测中在实践中并不能取得理想的结果,因此沿用了之前的传统思路(使用selectivesearch 的方法选取候选区域),不过这并不代表regression不可行,之后的YOLO就是基于回归的思路进行物体location的,并大放异彩.</p>
<h5 id="思路二从图中取出若干候选区域并选出最优">思路二:从图中取出若干候选区域,并选出最优</h5>
<p>候选区域（Region Proposal</p>
<h4 id="r-cnn-selective-search-cnn-svm-结构">R-CNN （Selective Search + CNN + SVM 结构）</h4>
<p>R-CNN算是卷积神经网络在目标检测领域的开山之作,整体结构上R-CNN继承了已经较为成熟的Selective Search + DPM/HoG + SVM的目标检测框架.在其最大的突破点在于使用CNN作为特征提取器,代替传统了的已经到瓶颈的手工特征(DPM/HoG);并且经过预训练的CNN特征提取器(如在ImageNet上预训练的Vgg)在应用时只需在目标数据集上进行fine-tuning,降低了对数据集的要求,可以解决数据集不足的问题.</p>
<h5 id="工作流程与总体结构">工作流程与总体结构</h5>
<p>RCNN的运行可分为四个步骤,我把它简化为“找框,喂框,认框,修框”,具体来说就是</p>
<blockquote>
<p>找框(生成候选区域)： 采用Selective Search 方法,对每张输入图片生成1-2K个候选区域</p>
<p>喂框(CNN特征提取)： 对每个候选区域，使用CNN提取特征,得到其特征向量</p>
<p>认框(类别判断)： 将候选区域的特征向量特征送入每一个类(人、马、狗 e.t.c.)的SVM 分类器，判别是否属于该类</p>
<p>修框(位置精修)： 使用回归器精细修正候选框位置</p>
</blockquote>
<p><img src="/2019/11/30/rcnn/figure/rcnn-process.png" alt="rcnn-process" style="zoom:50%;"></p>
<p>相应地,RCNN由三部分构成,第一部分是生成候选区域的Selective Search算法;第二部分是用来提取特征的CNN(文中选取了Vgg16),第三部分是用以分类的SVM.</p>
<h5 id="selective-search">Selective Search</h5>
<p>前面提到,RCNN是在之前的传统目标检测框架上迭代得到的,因此生成候选区域时仍然沿用了之前已经较为成熟的Selective Search方法.Selective Search主要是根据图像的色彩与纹理特征,先得到若干个较小的同类区域,再根据一定规则将较小的同类区域合,并得到物体的候选区域,其具体的算法流程如下</p>
<blockquote>
<ol type="1">
<li>根据图像的色彩纹理特征,使用过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，即得到了RCNN所需的候选区域</li>
</ol>
</blockquote>
<p>并且合并规则保证优先合并以下四种区域：</p>
<blockquote>
<p>1.颜色（颜色直方图）相近的</p>
<p>2.纹理（梯度直方图）相近的</p>
<p>3.合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域</p>
<p>4.合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</p>
</blockquote>
<p><img src="/2019/11/30/rcnn/figure/selectivesearch.png"></p>
<h4 id="spp-netroi-pooling-的引入">SPP-net（ROI Pooling 的引入）</h4>
<p>要优化得到RCNN的后续改进型,就要先看看RCNN有哪些缺点,之前RCNN在选出Selective Search上千个bondingbox之后,需要将每个物体框resize后依次通过CNN特征提取器,这是相当耗时的一个环节.如果可以先得到整张图片的特征,在每次选取bondingbox时取整体特征的一部分直接得到物体的特征值,效率必然会大大提升</p>
<p><img src="https://bloglunit.files.wordpress.com/2017/05/20170525-research-seminar-google-slides-2017-05-26-17-14-03.png"></p>
<p>SSP Net就一定程度上的解决了以上问题</p>
<p>1.结合空间金字塔方法实现CNNs的对尺度输入。 一般CNN后接全连接层或者分类器，他们都需要固定的输入尺寸，因此不得不对输入数据进行crop或者warp，这些预处理会造成数据的丢失或几何的失真。SPP Net的第一个贡献就是将金字塔思想加入到CNN，实现了数据的多尺度输入。</p>
<p>如下图所示，在卷积层和全连接层之间加入了SPP layer。此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出尺度始终是固定的。</p>
<p>2.只对原图提取一次卷积特征 在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。 所以SPP Net根据这个缺点做了优化：只对原图进行一次卷积得到整张图的feature map，然后找到每个候选框zaifeature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层。节省了大量的计算时间，比R-CNN有一百倍左右的提速。</p>
<p>Spatial Pyramid Pooling（空间金字塔池化）</p>
<h4 id="fast-r-cnnselective-search-cnn-roi-结构">Fast R-CNN（Selective Search + CNN + ROI 结构）</h4>
<p>SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在RCNN的基础上采纳了SPP Net方法，对RCNN作了改进，使得性能进一步提高。</p>
<p>R-CNN与Fast RCNN的区别有哪些呢？ 先说RCNN的缺点：即使使用了selective search等预处理步骤来提取潜在的bounding box作为输入，但是RCNN仍会有严重的速度瓶颈，原因也很明显，就是计算机对所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。</p>
<p><img src="/2019/11/30/rcnn/figure/fastrcnn.png" alt="fastrcnn" style="zoom:30%;"></p>
<p>大牛提出了一个可以看做单层sppnet的网络层，叫做ROI Pooling，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。另外，之前RCNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression，而在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal+CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster-RCNN做下了铺垫。</p>
<p><img src="https://bloglunit.files.wordpress.com/2017/05/20170525-research-seminar-google-slides-2017-05-29-19-06-26.png"></p>
<h4 id="faster-r-cnnrpn-cnn-roi-结构">Faster R-CNN（RPN + CNN + ROI 结构）</h4>
<p>Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？ 解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。 做这样的任务的神经网络叫做Region Proposal Network(RPN)。</p>
<p>具体做法： 　　• 将RPN放在最后一个卷积层的后面 　　• RPN直接训练得到候选区域</p>
<h5 id="rpn">RPN</h5>
<p>总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于Region Proposal的R-CNN系列目标检测方法是当前目标检测技术领域最主要的一个分支。</p>
<h4 id="reference">Reference</h4>
<ul>
<li>https://www.cnblogs.com/skyfsm/p/6806246.html</li>
<li>https://www.cnblogs.com/yhyue/p/9247962.html</li>
<li>https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e?gi=ca4fe493ba3e</li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> paper reading </tag>
            
            <tag> CV </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」Autoencoder as Assistant Supervisor Improving Text Representation for Chinese Social Media Text Summarization]]></title>
      <url>/2019/11/30/superAE/</url>
      <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1805.04869v1.pdf" target="_blank" rel="noopener">原文链接</a></p>
<h3 id="abstrct">Abstrct</h3>
<p>现今主流的摘要生成都是基于seq2seq,但事实上用于训练的文本大都过于冗杂(即使LCSTS也存在这个问题)导致在在encode时很难真正的“习得”语义(semantic)而reference summary大都短且语义明确。文中提出的模型「SuperAE」正是利用了这一点,在encode时引入了用于监督的Autocoder取得了,很好的结果</p>
<a id="more"></a>
<h3 id="introduction">Introduction</h3>
<p>由于RNN结构的特性,不管作何变种和优化(LSTM&amp;GRU)都难以避免梯度爆炸和消失,因此可以说编码长文本时语义的遗失是一种必然.而对于短文本的编码则可以很好的理解语义.根据这个思路,可以在编码时增加对summary的编码,用它来监督encoder对原文的的&quot;理解&quot;情况</p>
<h3 id="proposed-model">Proposed Model</h3>
<h4 id="supervision-with-autoencoder">Supervision with Autoencoder</h4>
<p>传统的seq2seq在这就不多赘述,encode层还是主流的双层LSTM,在训练时,加入了新的摘要编码器,输出为<span class="math inline">\(z_s\)</span>,引入了新的损失项(<span class="math inline">\(\lambda\)</span>有经验值0.3作为初始值)</p>
<p>$ L_s =d(z_t,z_s) $</p>
<p>其中</p>
<p>$ d(z_t,z_s) = ||z_t - z_s||_{2} $</p>
<p>显而易见此监督(supervisor)项用以描述二者输出的相似度。按我从直觉上的理解,监督项,可以很好的防止模型不被比较晦涩或者说质量较差的摘要样例带偏</p>
<p><img src="/2019/11/30/superAE/ae_graph.png"></p>
<h4 id="adversarial-learning">Adversarial Learning</h4>
<p>上一节添加了用于监督的新损失项,因此就存在了新的超参数<span class="math inline">\(\lambda\)</span>用来控制监督的力度.显而易见,训练时,如果摘要和原文的语义相关性很高，那么监督的力度应该较高,反之,如果摘要太草了(QAQ).就应该适当降低监督的惩罚力度　因此，如果训练时的惩罚力度是动态的，训练的效果当然会更好。所以我们需要一种技巧来判别这组文本和摘要的相关性，是否需要“加大力度”</p>
<p>到这里,就该对抗学习(adversarial learning)出场了.本文先验的把seq2seq里的输出看作虚假表示(&quot;fake&quot;representation),autoencoder的输出看作标准表示(&quot;gold&quot;representation) 对此,在训练中引入了discriminator(判别器?歧视器?)用来分辨输出到底是&quot;gold&quot;还是&quot;fake&quot;</p>
<p>从数学上理解，这是判别器的目标函数</p>
<p>$ L_D(<em>D) = -logP</em>{<em>D}(y =1|z_t)-logP</em>{_D}(y = 0|z_s) $</p>
<p>这是监督学习的目标函数</p>
<p>$ L_G(<em>E) = -logP</em>{<em>D}(y =0|z_t)-logP</em>{_D}(y = 1|z_s) $</p>
<p>从直觉上理解,监督学习有着使两编码器的输出语义无限接近的动机,而判别器有尽力分别二者的动机,二者都存于损失函数中,如果辨别器可以区分二者<span class="math inline">\(\lambda\)</span>减小,减轻监督力度,反之<span class="math inline">\(\lambda\)</span>增加,加大力度</p>
<p>这里看的有点迷，把原文po上来吧</p>
<blockquote>
<p>the supervision, which minimizes the dis-tance of the representations and makes them sim-ilar, tries to prevent the discriminator from mak-ing correct predictions.</p>
</blockquote>
<h3 id="loss-function-and-training">Loss Function and Training</h3>
<p>loss-function共由三部分组成,第一部分是原本seq2seq decoder和autoencoder均经过decod后输出的交叉熵的和,第二部分是上文第一节监督项的损失,第三部分是上文第二节判别器的损失</p>
<p>$ L_1 = L_{seq2seq} + L_{AE} + L_s + L_D + L_G $</p>
<figure>
<img src="/2019/11/30/superAE/result.png" alt="result"><figcaption>result</figcaption>
</figure>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> paper reading </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
