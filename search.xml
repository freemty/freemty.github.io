<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[(Paper-Reading) superAE]]></title>
      <url>/2019/11/30/superAE/</url>
      <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1805.04869v1.pdf" target="_blank" rel="noopener">Original-version Link</a></p>
<h3 id="Abstrct"><a href="#Abstrct" class="headerlink" title="Abstrct"></a>Abstrct</h3><p>现今主流的摘要生成都是基于seq2seq，但事实上用于训练的文本大都过于冗杂（即使LCSTS也存在这个问题），导致在在encode时很难真正的“习得”语义（semantic），而reference summary大都短且语义明确。文中提出的模型「SuperAE」正是利用了这一点，在encode时引入了监督。取得了，很好的结果。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于RNN结构的特性，不管作何变种和优化（LSTM，GRU），都难以避免梯度爆炸和消失，因此可以说编码长文本时语义的遗失是一种必然。而对于短文本的编码则可以很好的理解语义。根据这个思路，可以在编码时增加对summary的编码，用它来监督encoder对原文的的“理解”情况。</p>
<h3 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h3><h4 id="Supervision-with-Autoencoder"><a href="#Supervision-with-Autoencoder" class="headerlink" title="Supervision with Autoencoder"></a>Supervision with Autoencoder</h4><p>传统的seq2seq在这就不多赘述，encode层还是主流的双层LSTM，在训练时，加入了新的摘要编码器，输出为$z_s$,引入了新的损失项($\lambda$有经验值0.3作为初始值）<br>$$<br> L_s =\frac{\lambda}{N_h}d(z_t,z_s)<br>$$<br>其中<br>$$<br>d(z_t,z_s) = ||z_t - z_s||_2<br>$$</p>
<p>显而易见此监督（supervisor）项用以描述二者输出的相似度。按我从直觉上的理解，监督项，可以很好的防止模型不被比较晦涩或者说质量较差的摘要样例带偏。</p>
<p><img src="/2019/11/30/superAE/ae_graph.png" alt="model"></p>
<h4 id="Adversarial-Learning"><a href="#Adversarial-Learning" class="headerlink" title="Adversarial Learning"></a>Adversarial Learning</h4><p>上一节添加了用于监督的新损失项，因此就存在了新的超参数$\lambda$用来控制监督的力度。显而易见，训练时，如果摘要和原文的语义相关性很高，那么监督的力度应该较高，反之，如果摘要太草了(QAQ),就应该适当降低监督的惩罚力度。因此，如果训练时的惩罚力度是动态的，训练的效果当然会更好。所以我们需要一种技巧来判别这组文本和摘要的相关性，是否需要“加大力度”。</p>
<p>到这里，就该对抗学习（adversarial learning）出场了。本文先验的把seq2seq里的输出看作虚假表示（“fake”representation），autoencoder的输出看作标准表示（“gold” representation）。对此，在训练中引入了discriminator（判别器？歧视器？），用来分辨输出到底是“gold”还是“fake”。</p>
<p>从数学上理解，这是判别器的目标函数<br>$$<br>L_D(\theta_D) = -logP_{\theta_D}(y  =1|z_t)-logP_{\theta_D}(y = 0|z_s)<br>$$<br>这是监督学习的目标函数<br>$$<br>L_G(\theta_E) = -logP_{\theta_D}(y  =0|z_t)-logP_{\theta_D}(y = 1|z_s)<br>$$<br>从直觉上理解，监督学习有着使两编码器的输出语义无限接近的动机，而判别器有尽力分别二者的动机，二者都存于损失函数中，如果辨别器可以区分二者，$\lambda$减小，减轻监督力度，反之$\lambda$增加，加大力度。</p>
<p>这里看的有点迷，把原文po上来把</p>
<blockquote>
<p>the supervision, which minimizes the dis-tance of the representations and makes them sim-ilar, tries to prevent the discriminator from mak-ing correct predictions.</p>
</blockquote>
<h3 id="Loss-Function-and-Training"><a href="#Loss-Function-and-Training" class="headerlink" title="Loss Function and Training"></a>Loss Function and Training</h3><p>loss-function共由三部分组成，第一部分是原本seq2seq decoder和autoencoder均经过decod后输出的交叉熵的和，第二部分是上文第一节监督项的损失，第三部分是上文第二节判别器的损失。</p>
<p>$$<br>L_1 = L_{seq2seq} + L_{AE} + L_s + L_D + L_G<br>$$</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/2019/11/30/superAE/result.png" alt="results"></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2019/11/30/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hexo server</span></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hexo generate</span></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      
        
    </entry>
    
  
  
</search>
