<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[sum的RL学习日记1]]></title>
      <url>/2020/01/28/RL1/</url>
      <content type="html"><![CDATA[<h3 id="notations">Notations</h3>
<ul>
<li><span class="math inline">\(P(s&#39;, r \vert s, a)\)</span></li>
<li><span class="math inline">\(\pi(a \vert s)\)</span> agent的行为策略;</li>
<li><span class="math inline">\(G_t\)</span>累计回报 <span class="math inline">\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)</span></li>
<li><span class="math inline">\(V^\pi(s)\)</span> 采取策略<span class="math inline">\(\pi\)</span>时,状态<span class="math inline">\(s\)</span>下的累计回报;也可写作,<span class="math inline">\(Q(s)\)</span> <span class="math inline">\(V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s]\)</span></li>
<li><span class="math inline">\(Q^\pi(s, a)\)</span> 采取策略<span class="math inline">\(\pi\)</span>时，状态<span class="math inline">\(s\)</span>下，执行<span class="math inline">\(a\)</span>的累计回报<span class="math display">\[Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a]\]</span></li>
<li><span class="math inline">\(A(s, a)\)</span> advantage function,定义为<span class="math inline">\(A(s, a) = Q(s, a) - V(s)\)</span></li>
</ul>
<h3 id="从一根杠杆开始">从一根杠杆开始</h3>
<p>google不愧人类之光(暴论)！openAI的gym库里面提供了很多的虚拟环境，用于强化学习的训练和开发，而<a href="http://gym.openai.com/docs/" target="_blank" rel="noopener">CartPole</a>绝对可说是RL界的HelloWorld。它的规则很简单，在一个存在重力的环境下，智能体（agent）在每一时刻做出左移或者右移的action，保持杠杆倾斜不超过15度。我们的目标是让杠杆存活尽可能长的时间。</p>
<h3 id="policy-gradient">Policy Gradient</h3>
<h4 id="abstract">Abstract</h4>
<p>强化学习旨在帮助我们找到能获得最大回报的策略，而作为强化学习的一种重要方法，policy gradient直接对策略进行建模与优化，它的reward function被定义为 <span class="math display">\[
J(\theta)
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s)
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
\]</span> 好吧我承认，上面的话主要的目的在于提升本文逼格，对初学者理解policy gradient并没有太大帮助，下面我来试着用自己的理解来谈谈policy gradient</p>
<h4 id="我来给翻译翻译">我来给翻译翻译</h4>
<p>在传统的监督学习中，假使我们有足够多且理想的“正确答案”作为label，那么直接将交叉熵的结果进行反传，就可以拟合出足够好的模型。但在很多决策问题上，我们根本无法得到什么有效的label，就像在CartPole游戏中，我们的训练是没有任何数据集作为参考的。</p>
<p>其实可以把这类把任务看成像一站到底一样的知识竞赛，答错一定数量的题目即算出局。在我看来，传统的的监督学习就是搜集能到所有有题库和答案的选手，他们可以一遍一遍的背题库，最后的无限接近全对。而policy gradient下的的选手则是一次又一次的参加竞赛，没有答案可供参考，游戏结束后他获得的唯一信息也只是知道自己活了多少轮。</p>
<p>现在我们再回头，看看开头的的式子，似乎也就有了头绪，正如齐名，policy gradient就是对全局的policy进行建模与优化，具体在CartPole游戏里，它也是在一局游戏结束，才进行一次“复盘”，而它的目标函数也是由两部分相乘得到，第一部分是最大似然的交叉熵，第二部分是所有时刻action的完全回报。</p>
<p>对于第一部分，我们自不会陌生，它评估的是在特定state下，模型的预测和实际action的偏差程度，在这里体现的就是模型对自己选择的确信程度，在只考虑这部分目标函数的情况下，优化目标函数，即是提供了一个正反馈，这个模型越来越相信自己在最初状态作出的决策（这里说的不是很严谨，只是我自己的理解），越来越“刚愎自用”，这自然是不行的。</p>
<p>这里就需要引入reward了，在我看来，reward相当于实在给每个action打分，它相当于告诉了模型，那些个特定状态下随机的action中，到底有多少可取的部分。</p>
<p>根据有针对性的reward进行反传，很快就可以拟合出不错的策略。听起来很理想，但这就引出了下一个问题，也就是policy gradient的核心，到底如何得到reward？</p>
<p>在<code>CartPoleV1</code>这个环境里，对于单步reward的定义非常简单，游戏中agent每次action的reward都是1，听起来这个单步的reward好像毛用都没有，这就要引出一个很重要的概念discounted future reward（好像可以翻译成完全回报）计作<span class="math inline">\(G_t\)</span></p>
<h4 id="babymath">babyMath</h4>
<p>接下来，有一些babyMath，不过也只是符号比较唬人，并没有劝退部分（确信）</p>
<p>现在，我们重新捡回reward function那个让人嫌弃的定义式</p>
<p><span class="math display">\[
J(\theta)
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s)
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)
\]</span></p>
<p>数学层面，agent的整个游戏过程自然可以理解为采取<span class="math inline">\(\pi_\theta\)</span>策略时的Markov chain（马尔可夫链），而假使我们沿着Markov chain无限的延伸下去，就可以得到马尔可夫过程的平稳分布（stationary distribution）即：</p>
<p><span class="math display">\[
d^\pi(s) = \lim_{t \to \infty} P(s_t = s \vert s_0, \pi_\theta)
\]</span></p>
<p>接下来是喜闻乐见的微分环节</p>
<p><span class="math display">\[
% &lt;![CDATA[
\begin{aligned}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\
&amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)
\end{aligned} %]]&gt;
\]</span></p>
<p>进行化简 <span class="math display">\[
% &lt;![CDATA[
\begin{aligned}
&amp; \nabla_\theta V^\pi(s) \\
=&amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp; \scriptstyle{\text{根据全概率公式}}\\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) &amp; \scriptstyle{\text{乘法法则}} \\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s&#39;, r} P(s&#39;,r \vert s,a)(r + V^\pi(s&#39;))} \Big) &amp; \scriptstyle{\text{; Extend } Q^\pi \text{ with future state value.}} \\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s&#39;, r} P(s&#39;,r \vert s,a) \nabla_\theta V^\pi(s&#39;)} \Big) &amp; \scriptstyle{P(s&#39;,r \vert s,a) \text{ or } r \text{ is not a func of }\theta}\\
=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s&#39;} P(s&#39; \vert s,a) \nabla_\theta V^\pi(s&#39;)} \Big) &amp; \scriptstyle{\text{; Because }  P(s&#39; \vert s, a) = \sum_r P(s&#39;, r \vert s, a)}
\end{aligned} %]]&gt;
\]</span> 从红色部分可以看出，上式是个套娃 为了理解reward，需要理解agent的状态转移。 <span class="math display">\[
s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s&#39; \xrightarrow[]{a \sim \pi_\theta(.\vert s&#39;)} s&#39;&#39; \xrightarrow[]{a \sim \pi_\theta(.\vert s&#39;&#39;)} \dots
\]</span></p>
<p>假设一局游戏可以抽象化为agent采取<span class="math inline">\(\pi_\theta\)</span>，经过<span class="math inline">\(k\)</span>步，从状态<span class="math inline">\(s\)</span>转移到状态<span class="math inline">\(x\)</span>,计作<span class="math inline">\(\rho^\pi(s \to x, k)\)</span></p>
<p><span class="math display">\[
\rho^\pi(s \to x, k+1) = \sum_{s&#39;} \rho^\pi(s \to s&#39;, k) \rho^\pi(s&#39; \to x, 1)
\]</span></p>
<ul>
<li><span class="math inline">\(k=0\)</span>时，相当于agent什么都没做，自然有：<span class="math inline">\(\rho^\pi(s \to s, k=0) = 1\)</span></li>
<li><span class="math inline">\(k=1\)</span>时，根据贝叶斯公式：<span class="math inline">\(\rho^\pi(s \to s&#39;, k=1) = \sum_a \pi_\theta(a \vert s) P(s&#39; \vert s, a)\)</span></li>
<li>推广到普遍情况，套娃出现了：<span class="math inline">\(\rho^\pi(s \to x, k+1) = \sum_{s&#39;} \rho^\pi(s \to s&#39;, k) \rho^\pi(s&#39; \to x, 1)\)</span></li>
</ul>
<p>现在我们记<span class="math inline">\(\phi(s) = \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)\)</span> 继续化简<span class="math inline">\(\nabla_\theta V^\pi(s)\)</span> <span class="math display">\[
% &lt;![CDATA[
\begin{aligned}
=&amp; \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s&#39;} P(s&#39; \vert s,a) \color{red}{\nabla_\theta V^\pi(s&#39;)}\\
=&amp; \phi(s) + \sum_{s&#39;} \sum_a \pi_\theta(a \vert s) P(s&#39; \vert s,a) \color{red}{\nabla_\theta V^\pi(s&#39;)} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \color{red}{\nabla_\theta V^\pi(s&#39;)} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \color{red}{[ \phi(s&#39;) + \sum_{s&#39;&#39;} \rho^\pi(s&#39; \to s&#39;&#39;, 1) \nabla_\theta V^\pi(s&#39;&#39;)]} \\
=&amp; \phi(s) + \sum_{s&#39;} \rho^\pi(s \to s&#39;, 1) \phi(s&#39;) + \sum_{s&#39;&#39;} \rho^\pi(s \to s&#39;&#39;, 2)\color{red}{\nabla_\theta V^\pi(s&#39;&#39;)} \scriptstyle{\text{ ; Consider }s&#39;\text{ as the middle point for }s \to s&#39;&#39;}\\
=&amp; \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)
\end{aligned} %]]&gt;
\]</span></p>
<p><span class="math display">\[
% &lt;![CDATA[
\begin{aligned}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta V^\pi(s_0) &amp; \scriptstyle{\text{; Starting from a random state } s_0} \\
&amp;= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &amp;\scriptstyle{\text{; Let }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\
&amp;= \sum_{s}\eta(s) \phi(s) &amp; \\
&amp;= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp; \scriptstyle{\text{;  } \eta(s), s\in\mathcal{S} \text{ to be a probability distribution.}}\\
&amp;= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)
&amp; \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ is stationary distribution.}}\\
&amp;= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &amp;\\
&amp;= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] &amp; \scriptstyle{\text{; Because } (\ln x)&#39; = 1/x}
\end{aligned} %]]&gt;
\]</span></p>
<p>很多种方法</p>
<figure>
<img src="/2020/01/28/RL1/general_form_policy_gradient.png" alt="mathods"><figcaption>mathods</figcaption>
</figure>
<h3 id="马尔可夫决策过程">马尔可夫决策过程</h3>
<p>RL的几乎所有任务都可抽象成马尔可夫过程（Markov Decision Process, MDP）</p>
<p>马尔可夫性质 看似上式中的<span class="math inline">\(s_{t+1}\)</span>是一个套娃，是由之前所有时刻的状态得到的，但这里有一个关键点，一旦<span class="math inline">\(s_t\)</span>已知，便可由<span class="math inline">\(s_t\)</span>独立推出<span class="math inline">\(s_{t+1}\)</span>，之前的状态都可以扔掉。因此NLP任务并不具备这样的马尔可夫性质(我猜)。</p>
<p>马尔可夫决策过程由<span class="math inline">\((S,A,P,R,\gamma)\)</span>组成，<span class="math inline">\(S\)</span>为有限的状态集,<span class="math inline">\(A\)</span> 为有限的动作集, <span class="math inline">\(P\)</span> 为状态转移概率, <span class="math inline">\(R\)</span>为回报函数, <span class="math inline">\(\gamma\)</span> 为折扣因子，用来计算累积回报。注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作。</p>
<p>下面这个式子就是马尔可夫过程的核心了。值得注意的是状态转移的过程中，假定我们在状态<span class="math inline">\(s\)</span>这个节点上，我们采取行动<span class="math inline">\(a\)</span>，状态转移为<span class="math inline">\(s&#39;\)</span>的概率不是0或1，而是一个随机变量。 <span class="math display">\[
p_{s\hat s}^a=p(\hat s|s,a)=p(S_{t+1}=\hat s| S_t=s,A_t=a)
\]</span>接下来就要引出累计回报（Return）<span class="math inline">\(G_t\)</span>了,定义式如下 $$</p>
<p><span class="math inline">\(\pi_t\)</span></p>
<p><span class="math display">\[
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
\]</span></p>
<h3 id="reference">Reference</h3>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-4-gym/" target="_blank" rel="noopener">OpenAI gym 环境库 by莫烦</a> <a href="https://tobiaslee.top/2018/03/06/Reinforcement-Learning1/" target="_blank" rel="noopener">Policy Graident 从数学到实现</a> <a href="https://zhuanlan.zhihu.com/p/25743759" target="_blank" rel="noopener">强化学习之蒙特卡罗方法</a> <a href="http://www.foolweel.com/2019/05/02/rl-basic/" target="_blank" rel="noopener">强化学习概述</a> <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Policy Gradient Algorithms</a></p>
]]></content>
      
        
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谈谈我们的“异化”]]></title>
      <url>/2019/12/05/suibi1/</url>
      <content type="html"><![CDATA[<p>些这篇文章的是前段时间的毛概大作业，虽然不长，但也费了不少心力。不得不说，写作过程还是相当痛苦的，不过也难得可以以这种自己“瞧得起”的方式表达真情实感。所以发在博客上，希望以后可以继续写作这个好习惯吧。</p>
<a id="more"></a>
<h3 id="这个时代的异化">这个时代的异化</h3>
<p>异化,代表着人对自己的劳动与外部世界丧失控制,甚至反被操纵的过程,在马克思的笔下,它是“人的受难”,而在现今的网络空间中,我们的情绪与表达也在越来越多的呈现着这样的异化.</p>
<h4 id="清醒的哲人王们">清醒的“哲人王”们</h4>
<p>格雷塔·桑伯格,瑞典的环保少女,因她极端反现代化的环保诉求与乖张的事迹而为人所知.2018年,15岁的她每个周五都会独自在瑞典国会前举着“为气候变化而罢课”抗议静坐.并参加了今年九月的联合国气候峰会,并因演讲中那张表情夸张的“How Dear You”而红遍中国的社交网络(这里的“红”纯粹是指其激发的情绪之剧烈).活脱一个完美的“妄人”范式.与“疯姑娘”一同被推到风口的是,是“普京一语戳破西方环保阴谋”,“中国的绿化成就震撼世界,为英雄们点赞”... 「说来戏谑,每当我们被西方白左气到时,总是要抬出那些“最可爱的人们”给我们撑腰」</p>
<p>说老实话,让我们用自己的直觉判断,也大概可以明白这位“疯姑娘”的诉求并不现实,许多乖张的事迹也确实逃不开炒作的嫌疑.可“不实际”,“不纯粹”难道就真的能作为我们情绪如此高亢的理由吗?如果“非蠢即坏”的咒骂真的源于我们的道德自觉,那么在网络上,我们对大屠杀,纳粹,红色高棉,又何曾有过如此纯粹的愤怒呢? 「主观的认为,在当今的网络中不管是大屠杀的恶,还是雷锋式的善都沦为了虚无的,关于善恶的“道理”;而网络中的高亢表达,根本上源于一种“恶感”」</p>
<p>在这里,人人都是了不起的智者,一眼就戳穿了反智的本质与西方的阴谋,用尽工整精致的语言,向人们证明着“我什么都不做”比“反智”高明多了,再顺带对那些真正真诚的实践者施以崇高的敬意.熟练的运用着自己的理性,歌颂着“伟大”,唾弃着“虚伪”,何其高明的“哲人王”们!</p>
<p>如果我说这么多只是为了讽刺所谓“乌合之众”,那就太过流于表面了,我想问的是,我们的愤怒与崇敬,是不是都有些太过轻巧了?在我看来这不仅仅是对于“妄人”的讽刺,更是对于“偶像”的羞辱.试问诸位,既然你们对那些投身环保的实践者们如此崇敬,请问,你们能拿出那位“疯姑娘”十分之一的热忱去做过哪怕一次你们认为“不虚伪”“有意义”的环保?我倒不妨直接说开了吧,你们口中的“崇高”“务实”的英雄们,不过是你们拿来搬弄是非的工具罢了.我们都特别明白行动和言辞的界限,从不轻易僭越到无能为力的境地.</p>
<pre><code>叶公子高好龙，钩以写龙，凿以写龙，屋室雕文以写龙。
于是天龙闻而下之,叶公见之，弃而还走，失其魂魄，五色无主。
是叶公非好龙也，好夫似龙而非龙者也。</code></pre>
<p>在我看来,这种过分“冷静”的情绪实际上源于一种实践的不可欲,现代社会无限体量与联通,让价值的丧失成了一种必然代价,在我们父辈或更早的时候,一个人在其所在的宗族国企大院这样的共同体中,你只需要在这个共同体或者大院里做到有一技以傍身,你就能获得充分的”价值感”,而在当今这个“无限”世界中的,你所做的一切,只要打开网络,看到那些“神迹”(乔丹,比尔盖茨,...),作为一个孤立的个体,还怎能真正的相信自己实践的价值「可能表达的有些极端,但确实有这样的倾向」.于是,在人们的言论与想象之中,个体的能量只剩零和无穷大,除非您能绝圣弃智,行出“神迹”,任我们顶礼膜拜.不然就请闭嘴.什么,你告诉我这也叫成就?五十步笑百步罢了,指不定背后还有什么不可告人的阴谋! 「或许是一种语言的遮蔽,在实践的层面上,“五十步”怎么就不能“笑百步”了?」</p>
<p>从人格上讲,“妄人”“英雄”于我们同样遥远.可我们羞于面对这种平庸,痛恨一切不能一蹴而就的改变,我们只能指望着几句精致的讽刺与假惺惺的热爱,证明自己真的与英雄们“心有戚戚焉”,对着光屏艰难着维护者一个“自洽”的我.激昂的言辞昭示的是无根之人的惶恐.以及对于独自实践的恐惧.</p>
<h4 id="庶民的胜利">庶民的胜利</h4>
<p>今年七月后,一个不那么善意的称呼出现在了网络上--“废青”,意指那些在香港“反送中”运动中参与“暴乱”的香港年轻人.而这个称呼也是用来讽刺他们在“自以为正确”的“暴乱”中诸多幼稚与滑稽的行为. 由于可能涉及一些政治敏感话题,我觉得在这里有必要申明一下,现在社会上所有发生的事,在网络这个“拟像”上激起的涟漪,从来都与事实真相无关.网络中普遍存在的“表达异化”,都是蕴藏在浪潮中不断共振的情绪而非事实的道理之中.因此我无意对所提及的事件做任何事实层面的判断,我所在乎的是“第四面墙”后观众席上的声音. 在撇清责任后,让我们再次回到这场“时代革命”的浪潮中.从七月中旬,无数的浪潮一波又一波的掀起,“帝吧百万出征”“阿中女孩为国护旗”......无数年轻人用着自己的方式“捍卫”着的价值. 在这次“全民爱国”的主阵地之一,新浪微博的“#中国”超话下,鲁迅五四时期《新青年》上的一段文章</p>
<ul>
<li>“愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。此后如竟没有炬火，我便是唯一的光”</li>
</ul>
<p>再次被广泛的引用转发,初听起来极富感情,仔细一想,越觉得不对.难道,这些在网路上“为国护旗,挺港警”的青年们,真的把自己带入到了那些五四学生的视角了吗? 在过去那些风云激荡的年代,我们抗议,写作,斗争,用着自己的热血与理想,表达着自己真实的诉求.同样,这一切“真”的行动,从来都意味着困难,风险,与代价.(我没有任何号召大家去模仿的意思,只是想说,真正的表达与捍卫,从来都是伴着付出与冒险的)</p>
<p>而时至今日,我不妨直说,在这场斗宏大运动中的每个青年,他们都心知肚明,自己胜券在握,他们不论说出如何过激的言语,都不用承担任何真实的代价,不论是武力的对抗还是舆论的斗争,“阿中”的胜利都只是时间问题.他们在这场根本不会波及自己真实生活的战场上肆意“杀敌”.</p>
<p>他们的慷慨陈词,与其说实在发自内心的捍卫立场,不如说是在胜券在握的的关头,毫无风险,毫无代价的享受着胜利的喜悦!如果说那些不辨菽麦走上街头的勇武派是“废青”,那么在网路上嗜胜的青年们,绝对是“强青”了!换一种极端的讲法,与其说他们是爱国,爱某些价值,不如说他们是爱“赢”罢了. 热血方刚的青年们,享受着毫无代价,毫无风险的“勇气”与“热情”,所带来的快感.可这终归是“假勇气”,“假热情”.我想问,在洞穴中沉溺于烛光倒影的囚徒们的,就算解开镣铐,还有没有胆量去直面阳光与这真实的世界呢?</p>
<h4 id="代庖之乐">“代庖”之乐</h4>
<p>如果说这些“假捍卫”“假勇气”的只是一种不自知的异化,那这场浪潮中的另一幅面向,则呈现出来一种呈现出来一种更深的异化与懦弱.</p>
<p>大概是八月中旬一组“深圳警方千人实战演习”的短视频刷爆了网络,最令我惊讶的是在评论中多数都是类似“年度大戏,即将上演”,“结局到来,敬请期待!”这类具有“幽默”与期待的讽刺.(毫无疑问大家都明白着这隐喻着什么)如果说帝吧出征与香港青年对骂还是算是自发行动去捍卫自己的立场的一场自维的“战斗”,那大家对“深圳警方演习”“驻港部队换防”这样的拍手称快,则纯粹是一种对于“代庖”的享受,我甚至可以极为武断的说,如果这些表述中有一个主导的情绪,那一定是“胜利的喜悦”.</p>
<p>「这样糟糕的情绪,自然与不断的对立与极端化报道脱不开干系,青年们想象中的敌人让早已非人化,不过可悲的这已不是什么新鲜事了」</p>
<p>伴随这种“代庖”,还有一个更糟糕的面向,那就是在网络各种场合“举报”这种表达形式的蔚然成风.在微博上许多明星的“反黑组”「意指粉丝中为了维护明星声誉而在网络上巡查监控负面言论的自组织形式」,每天都会定时贴出数十甚至上百的账号信息,用以让粉丝团体精确举报,通过官方封号来消除负面评价.可见我们对于这种“高效”的斗争形式适应的相当之快.</p>
<p>可以说,这种在网络世界上对于“代庖”的渴望,是我们面对如此庞大的现代社会与国家机器时,最舒适的获得“自洽”的方式.可是,“代庖”从来不是没有代价的,他的代价,就是纯粹的交托,纯粹的异化,放弃一切应然的执念,相信自己外部的一切都是“State of The Art”(当前最好的结果).才能心安理得. 最后我只想说,这种享受“代庖”的狂欢,已不仅仅是的异化表达,而是这个庞大现代社会所异化的虚弱之人唯一的表达罢了!</p>
<p><strong>写在最后</strong> 其实每当有这样“文以载道”的机会,我都会燃起不小的热情,想去说点什么,可当我真正的拿起笔,很多原来引以为傲的“灼见”,落到纸上,都只剩一些搬弄是非的文字游戏,读完尽是些言之无物的空谈. 这种挫败也带给了我一个新的视角,是不是在网络时代,我们的“言之无物”成为了一种必然.面对那么多富有煽动力的媒介内容源源不断的涌来,我们的观点变得毫无意义,我们只能说“好”与“坏”,或是运用“精致的文字游戏”去捍卫别人预设好的立场. 经历了一天极为痛苦的写作,也让我意识到了,写作是一个内在蕴含反思的过程,真正落到纸上的文字,才是经得起反思,蕴含着真实表达的.所以最后我想说,对这种表达的异化.我们并不是完全无力的,但终是要付出一些困难的.要摒弃这种异化,就要让我们的表达不再是,“搬弄是非”,承载着“假勇气”的“文字游戏”,少争吵,多写作. 文章不长,描述很多的现象其实也充满了我主观的臆想,不过我坚信,在这个“人人都是知识分子”的时代,真的情感远比精致的道理来的可贵,一如深刻的视角比最后真相更能让人反思,指引我们去真正的实践!</p>
]]></content>
      
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[「Paper-Reading」Autoencoder as Assistant Supervisor Improving Text Representation for Chinese Social Media Text Summarization]]></title>
      <url>/2019/11/30/superAE/</url>
      <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1805.04869v1.pdf" target="_blank" rel="noopener">原文链接</a></p>
<h3 id="abstrct">Abstrct</h3>
<p>现今主流的摘要生成都是基于seq2seq,但事实上用于训练的文本大都过于冗杂(即使LCSTS也存在这个问题)导致在在encode时很难真正的“习得”语义(semantic)而reference summary大都短且语义明确。文中提出的模型「SuperAE」正是利用了这一点,在encode时引入了用于监督的Autocoder取得了,很好的结果</p>
<a id="more"></a>
<h3 id="introduction">Introduction</h3>
<p>由于RNN结构的特性,不管作何变种和优化(LSTM&amp;GRU)都难以避免梯度爆炸和消失,因此可以说编码长文本时语义的遗失是一种必然.而对于短文本的编码则可以很好的理解语义.根据这个思路,可以在编码时增加对summary的编码,用它来监督encoder对原文的的&quot;理解&quot;情况</p>
<h3 id="proposed-model">Proposed Model</h3>
<h4 id="supervision-with-autoencoder">Supervision with Autoencoder</h4>
<p>传统的seq2seq在这就不多赘述,encode层还是主流的双层LSTM,在训练时,加入了新的摘要编码器,输出为<span class="math inline">\(z_s\)</span>,引入了新的损失项(<span class="math inline">\(\lambda\)</span>有经验值0.3作为初始值)</p>
<p>$ L_s =d(z_t,z_s) $</p>
<p>其中</p>
<p>$ d(z_t,z_s) = ||z_t - z_s||_{2} $</p>
<p>显而易见此监督(supervisor)项用以描述二者输出的相似度。按我从直觉上的理解,监督项,可以很好的防止模型不被比较晦涩或者说质量较差的摘要样例带偏</p>
<p><img src="/2019/11/30/superAE/ae_graph.png"></p>
<h4 id="adversarial-learning">Adversarial Learning</h4>
<p>上一节添加了用于监督的新损失项,因此就存在了新的超参数<span class="math inline">\(\lambda\)</span>用来控制监督的力度.显而易见,训练时,如果摘要和原文的语义相关性很高，那么监督的力度应该较高,反之,如果摘要太草了(QAQ).就应该适当降低监督的惩罚力度　因此，如果训练时的惩罚力度是动态的，训练的效果当然会更好。所以我们需要一种技巧来判别这组文本和摘要的相关性，是否需要“加大力度”</p>
<p>到这里,就该对抗学习(adversarial learning)出场了.本文先验的把seq2seq里的输出看作虚假表示(&quot;fake&quot;representation),autoencoder的输出看作标准表示(&quot;gold&quot;representation) 对此,在训练中引入了discriminator(判别器?歧视器?)用来分辨输出到底是&quot;gold&quot;还是&quot;fake&quot;</p>
<p>从数学上理解，这是判别器的目标函数</p>
<p>$ L_D(<em>D) = -logP</em>{<em>D}(y =1|z_t)-logP</em>{_D}(y = 0|z_s) $</p>
<p>这是监督学习的目标函数</p>
<p>$ L_G(<em>E) = -logP</em>{<em>D}(y =0|z_t)-logP</em>{_D}(y = 1|z_s) $</p>
<p>从直觉上理解,监督学习有着使两编码器的输出语义无限接近的动机,而判别器有尽力分别二者的动机,二者都存于损失函数中,如果辨别器可以区分二者<span class="math inline">\(\lambda\)</span>减小,减轻监督力度,反之<span class="math inline">\(\lambda\)</span>增加,加大力度</p>
<p>这里看的有点迷，把原文po上来吧</p>
<blockquote>
<p>the supervision, which minimizes the dis-tance of the representations and makes them sim-ilar, tries to prevent the discriminator from mak-ing correct predictions.</p>
</blockquote>
<h3 id="loss-function-and-training">Loss Function and Training</h3>
<p>loss-function共由三部分组成,第一部分是原本seq2seq decoder和autoencoder均经过decod后输出的交叉熵的和,第二部分是上文第一节监督项的损失,第三部分是上文第二节判别器的损失</p>
<p>$ L_1 = L_{seq2seq} + L_{AE} + L_s + L_D + L_G $</p>
<h3 id="results">Results</h3>
<p><img src="/2019/11/30/superAE/result.png"></p>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> paper reading </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
